{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3cdc422",
   "metadata": {},
   "source": [
    "**zzc_ail kısmı bu kodlar aracılığı ile yapıldı.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3f73d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 819us/step - loss: 0.0122\n",
      "Epoch 2/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 1.7933e-06\n",
      "Epoch 3/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - loss: 7.6221e-07\n",
      "Epoch 4/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.4084e-07\n",
      "Epoch 5/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.5971e-07\n",
      "Epoch 6/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.7161e-07\n",
      "Epoch 7/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.7412e-07\n",
      "Epoch 8/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 2.4705e-07\n",
      "Epoch 9/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.2063e-07\n",
      "Epoch 10/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.2261e-07\n",
      "Epoch 11/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.9180e-07\n",
      "Epoch 12/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.1377e-07\n",
      "Epoch 13/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.9525e-07\n",
      "Epoch 14/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.5193e-06\n",
      "Epoch 15/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.7375e-07\n",
      "Epoch 16/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.8358e-07\n",
      "Epoch 17/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.9692e-07\n",
      "Epoch 18/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.0565e-07\n",
      "Epoch 19/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.9873e-06\n",
      "Epoch 20/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.8617e-07\n",
      "Epoch 21/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6134e-06\n",
      "Epoch 22/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.4818e-07\n",
      "Epoch 23/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2442e-06\n",
      "Epoch 24/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1519e-06\n",
      "Epoch 25/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.1131e-07\n",
      "Epoch 26/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.1102e-07\n",
      "Epoch 27/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1929e-06\n",
      "Epoch 28/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.6680e-07\n",
      "Epoch 29/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.8018e-06\n",
      "Epoch 30/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.3587e-07\n",
      "Epoch 31/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1951e-07\n",
      "Epoch 32/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3.1545e-06\n",
      "Epoch 33/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5138e-06\n",
      "Epoch 34/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.4526e-06\n",
      "Epoch 35/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.7778e-08\n",
      "Epoch 36/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1773e-07\n",
      "Epoch 37/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2318e-06\n",
      "Epoch 38/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.7593e-07\n",
      "Epoch 39/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0940e-06\n",
      "Epoch 40/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.7537e-07\n",
      "Epoch 41/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4486e-06\n",
      "Epoch 42/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0347e-07\n",
      "Epoch 43/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.5939e-07\n",
      "Epoch 44/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.2728e-06\n",
      "Epoch 45/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0950e-07\n",
      "Epoch 46/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0090e-07\n",
      "Epoch 47/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.6189e-08\n",
      "Epoch 48/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1330e-07\n",
      "Epoch 49/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.7747e-06\n",
      "Epoch 50/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.7129e-07\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Gelecekteki Fiyat Tahminleri:\n",
      "[[23871.344]\n",
      " [23753.27 ]\n",
      " [23651.28 ]\n",
      " [23809.049]\n",
      " [24991.963]\n",
      " [24733.021]\n",
      " [25079.967]\n",
      " [25172.107]\n",
      " [24871.201]\n",
      " [25082.748]\n",
      " [25315.697]\n",
      " [24837.883]\n",
      " [24953.238]\n",
      " [24795.195]\n",
      " [25015.973]\n",
      " [25256.98 ]\n",
      " [25344.06 ]\n",
      " [25273.875]\n",
      " [25130.021]\n",
      " [25210.855]\n",
      " [24994.365]\n",
      " [24805.578]\n",
      " [24851.637]\n",
      " [24910.877]\n",
      " [24994.154]\n",
      " [24876.48 ]\n",
      " [24778.95 ]\n",
      " [24772.316]\n",
      " [24765.885]\n",
      " [24780.455]\n",
      " [24981.574]\n",
      " [25088.145]\n",
      " [25113.566]\n",
      " [25178.053]\n",
      " [25179.555]\n",
      " [25111.068]\n",
      " [25181.521]\n",
      " [25269.988]\n",
      " [25319.484]\n",
      " [25346.95 ]\n",
      " [25448.535]\n",
      " [25455.105]\n",
      " [25419.377]\n",
      " [25483.74 ]\n",
      " [25296.252]\n",
      " [25149.588]\n",
      " [25217.81 ]\n",
      " [25265.855]\n",
      " [25287.059]\n",
      " [25258.988]\n",
      " [25180.844]\n",
      " [25448.63 ]\n",
      " [25341.527]\n",
      " [25289.861]\n",
      " [25263.48 ]\n",
      " [25356.574]\n",
      " [25507.584]\n",
      " [25406.2  ]\n",
      " [25531.955]\n",
      " [25786.533]\n",
      " [25455.904]\n",
      " [25550.541]\n",
      " [25732.371]\n",
      " [25820.217]\n",
      " [25955.623]\n",
      " [26046.025]\n",
      " [25948.787]\n",
      " [25701.227]\n",
      " [25928.812]\n",
      " [25626.074]\n",
      " [25701.766]\n",
      " [25703.623]\n",
      " [25788.688]\n",
      " [25792.299]\n",
      " [26085.506]\n",
      " [26746.512]\n",
      " [26781.87 ]\n",
      " [26995.96 ]\n",
      " [26897.016]\n",
      " [26909.578]\n",
      " [27074.371]\n",
      " [26918.666]\n",
      " [27158.908]\n",
      " [27095.088]\n",
      " [27291.162]\n",
      " [27277.44 ]\n",
      " [27276.32 ]\n",
      " [27088.95 ]\n",
      " [27096.646]\n",
      " [27274.752]\n",
      " [27222.357]\n",
      " [27109.379]\n",
      " [27080.688]\n",
      " [27150.447]\n",
      " [26930.646]\n",
      " [27004.494]\n",
      " [27179.621]\n",
      " [26969.404]\n",
      " [27055.475]\n",
      " [27022.166]\n",
      " [27035.557]\n",
      " [27094.559]\n",
      " [27126.682]\n",
      " [26936.21 ]\n",
      " [26818.732]\n",
      " [26398.543]\n",
      " [26557.182]\n",
      " [26682.508]\n",
      " [26697.25 ]\n",
      " [26567.094]\n",
      " [26506.072]\n",
      " [26643.422]\n",
      " [26786.342]\n",
      " [26857.982]\n",
      " [27110.803]\n",
      " [27050.838]\n",
      " [27100.887]\n",
      " [27163.854]\n",
      " [27147.258]\n",
      " [27356.756]\n",
      " [27415.822]\n",
      " [27635.863]\n",
      " [27582.988]\n",
      " [27555.629]\n",
      " [27493.754]\n",
      " [27401.088]\n",
      " [27156.578]\n",
      " [27231.318]\n",
      " [27343.23 ]\n",
      " [27479.996]\n",
      " [27532.678]\n",
      " [27622.541]\n",
      " [27463.475]\n",
      " [27576.043]\n",
      " [27448.834]\n",
      " [27516.258]\n",
      " [27406.219]\n",
      " [27442.238]\n",
      " [27480.22 ]\n",
      " [27567.787]\n",
      " [27475.6  ]\n",
      " [27525.951]\n",
      " [27689.45 ]\n",
      " [27558.957]\n",
      " [27605.277]\n",
      " [27461.03 ]\n",
      " [27431.576]\n",
      " [27420.547]\n",
      " [27033.71 ]\n",
      " [26952.512]\n",
      " [26626.805]\n",
      " [26800.555]\n",
      " [26704.191]\n",
      " [26351.137]\n",
      " [26058.736]\n",
      " [26559.285]\n",
      " [26720.146]\n",
      " [26894.479]\n",
      " [26839.03 ]\n",
      " [27030.   ]\n",
      " [26920.787]\n",
      " [25949.826]\n",
      " [25640.41 ]\n",
      " [25342.227]\n",
      " [25803.71 ]\n",
      " [25716.193]\n",
      " [25203.31 ]\n",
      " [25473.576]\n",
      " [25278.988]\n",
      " [25456.709]\n",
      " [25514.707]\n",
      " [25771.562]\n",
      " [25797.584]\n",
      " [25621.287]\n",
      " [25529.914]\n",
      " [25649.312]\n",
      " [26395.55 ]\n",
      " [25934.72 ]\n",
      " [25607.486]\n",
      " [25655.39 ]\n",
      " [25669.967]\n",
      " [25722.031]\n",
      " [25288.902]\n",
      " [25472.633]\n",
      " [25542.414]\n",
      " [25826.389]\n",
      " [25746.607]\n",
      " [25637.082]\n",
      " [25567.078]\n",
      " [25500.55 ]\n",
      " [25597.553]\n",
      " [25764.291]\n",
      " [25396.95 ]\n",
      " [25179.342]\n",
      " [25373.682]\n",
      " [25241.191]\n",
      " [25340.871]\n",
      " [25195.248]\n",
      " [25180.865]\n",
      " [25420.088]\n",
      " [25459.646]]\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "RMSE: 0.00048756292988456283\n",
      "MAE: 0.00035567596396486546\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "veri = pd.read_csv(r\"C:/Users/ASUS/Desktop/AI CLUB DATATHON/CSV/zzc_ail.csv\")\n",
    "\n",
    "veri['open'] = veri['open'].str.replace('\"', '').astype(float)\n",
    "veri['high'] = veri['high'].str.replace('\"', '').astype(float)\n",
    "veri['low'] = veri['low'].str.replace('\"', '').astype(float)\n",
    "veri['close'] = veri['close'].str.replace('\"', '').astype(float)\n",
    "\n",
    "veri['open'].fillna(veri['open'].mean(), inplace=True)\n",
    "veri['high'].fillna(veri['high'].mean(), inplace=True)\n",
    "veri['low'].fillna(veri['low'].mean(), inplace=True)\n",
    "veri['close'].fillna(veri['close'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "veri = veri[['open', 'low', 'high', 'close']]\n",
    "\n",
    "X = veri[['open', 'high', 'close']].values\n",
    "y = veri['open'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_scaled, y_scaled, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "gelecek_veri = X_scaled[-201:] \n",
    "tahminler = model.predict(gelecek_veri)\n",
    "\n",
    "tahminler = scaler.inverse_transform(tahminler)\n",
    "\n",
    "print(\"Gelecekteki Fiyat Tahminleri:\")\n",
    "print(tahminler)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dadcb2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0126\n",
      "Epoch 2/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.0413e-06\n",
      "Epoch 3/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1935e-06\n",
      "Epoch 4/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.6653e-06\n",
      "Epoch 5/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.1726e-06\n",
      "Epoch 6/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.7359e-06\n",
      "Epoch 7/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0367e-06\n",
      "Epoch 8/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5843e-06\n",
      "Epoch 9/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.7682e-06\n",
      "Epoch 10/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0920e-06\n",
      "Epoch 11/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2651e-06\n",
      "Epoch 12/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3778e-06\n",
      "Epoch 13/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3147e-06\n",
      "Epoch 14/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.6282e-07\n",
      "Epoch 15/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.8652e-06\n",
      "Epoch 16/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.7717e-07\n",
      "Epoch 17/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.9848e-06\n",
      "Epoch 18/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.4578e-07\n",
      "Epoch 19/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.3957e-07\n",
      "Epoch 20/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.3102e-06\n",
      "Epoch 21/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.9311e-07\n",
      "Epoch 22/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0619e-06\n",
      "Epoch 23/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.0312e-07\n",
      "Epoch 24/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0719e-06\n",
      "Epoch 25/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.7251e-07\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Gelecekteki Fiyat Tahminleri:\n",
      "[[24046.928]\n",
      " [23932.057]\n",
      " [23889.87 ]\n",
      " [24349.926]\n",
      " [25069.434]\n",
      " [25149.174]\n",
      " [25482.625]\n",
      " [25373.389]\n",
      " [25160.959]\n",
      " [25360.232]\n",
      " [25418.852]\n",
      " [25101.629]\n",
      " [25337.133]\n",
      " [25122.69 ]\n",
      " [25308.984]\n",
      " [25388.154]\n",
      " [25438.852]\n",
      " [25361.744]\n",
      " [25426.988]\n",
      " [25249.803]\n",
      " [25059.016]\n",
      " [24967.352]\n",
      " [24968.21 ]\n",
      " [25018.826]\n",
      " [25049.662]\n",
      " [25016.379]\n",
      " [24931.635]\n",
      " [24924.873]\n",
      " [24947.703]\n",
      " [25048.5  ]\n",
      " [25261.576]\n",
      " [25212.977]\n",
      " [25237.479]\n",
      " [25350.605]\n",
      " [25298.229]\n",
      " [25263.283]\n",
      " [25609.81 ]\n",
      " [25480.148]\n",
      " [25453.895]\n",
      " [25648.027]\n",
      " [25575.006]\n",
      " [25596.135]\n",
      " [25643.037]\n",
      " [25502.871]\n",
      " [25453.936]\n",
      " [25318.05 ]\n",
      " [25311.266]\n",
      " [25361.947]\n",
      " [25358.121]\n",
      " [25308.246]\n",
      " [25494.352]\n",
      " [25514.73 ]\n",
      " [25434.848]\n",
      " [25436.016]\n",
      " [25429.379]\n",
      " [11132.418]\n",
      " [25557.479]\n",
      " [11130.018]\n",
      " [25899.273]\n",
      " [26421.729]\n",
      " [25608.994]\n",
      " [25961.127]\n",
      " [25962.99 ]\n",
      " [25991.516]\n",
      " [26191.492]\n",
      " [26097.484]\n",
      " [25979.248]\n",
      " [25980.521]\n",
      " [25974.105]\n",
      " [25772.209]\n",
      " [11133.313]\n",
      " [25864.662]\n",
      " [25928.475]\n",
      " [26129.412]\n",
      " [26854.605]\n",
      " [27003.566]\n",
      " [27066.006]\n",
      " [27063.89 ]\n",
      " [27229.164]\n",
      " [27205.473]\n",
      " [27272.914]\n",
      " [27275.754]\n",
      " [27403.023]\n",
      " [27368.988]\n",
      " [27355.873]\n",
      " [27375.992]\n",
      " [27443.08 ]\n",
      " [27241.29 ]\n",
      " [27356.72 ]\n",
      " [27422.012]\n",
      " [27296.762]\n",
      " [27266.268]\n",
      " [27307.887]\n",
      " [27211.633]\n",
      " [27140.678]\n",
      " [27153.809]\n",
      " [27262.057]\n",
      " [27169.418]\n",
      " [27129.303]\n",
      " [27222.326]\n",
      " [27140.877]\n",
      " [27175.143]\n",
      " [27382.564]\n",
      " [27040.72 ]\n",
      " [26890.078]\n",
      " [26538.996]\n",
      " [11128.871]\n",
      " [26836.416]\n",
      " [26738.725]\n",
      " [26665.18 ]\n",
      " [26722.549]\n",
      " [26841.928]\n",
      " [27033.307]\n",
      " [27166.58 ]\n",
      " [27160.111]\n",
      " [27216.842]\n",
      " [27212.84 ]\n",
      " [27211.838]\n",
      " [27412.395]\n",
      " [27589.861]\n",
      " [27765.088]\n",
      " [27745.398]\n",
      " [27664.535]\n",
      " [27688.316]\n",
      " [27582.492]\n",
      " [27515.088]\n",
      " [27485.168]\n",
      " [27559.348]\n",
      " [27532.494]\n",
      " [27597.326]\n",
      " [27679.152]\n",
      " [27694.021]\n",
      " [27674.209]\n",
      " [27624.672]\n",
      " [27573.824]\n",
      " [27584.559]\n",
      " [27499.799]\n",
      " [27600.336]\n",
      " [27635.666]\n",
      " [27760.541]\n",
      " [27583.164]\n",
      " [27749.43 ]\n",
      " [27814.35 ]\n",
      " [27724.78 ]\n",
      " [27710.96 ]\n",
      " [27523.668]\n",
      " [27530.555]\n",
      " [27463.398]\n",
      " [27310.78 ]\n",
      " [27130.41 ]\n",
      " [26876.334]\n",
      " [11126.783]\n",
      " [26723.309]\n",
      " [26503.502]\n",
      " [26777.01 ]\n",
      " [26768.312]\n",
      " [26983.33 ]\n",
      " [27006.377]\n",
      " [27277.068]\n",
      " [27134.006]\n",
      " [26956.426]\n",
      " [26122.82 ]\n",
      " [25763.47 ]\n",
      " [25686.469]\n",
      " [25913.932]\n",
      " [25795.492]\n",
      " [25598.705]\n",
      " [25619.457]\n",
      " [25539.871]\n",
      " [11129.792]\n",
      " [25851.457]\n",
      " [25842.572]\n",
      " [11126.219]\n",
      " [25762.377]\n",
      " [11129.691]\n",
      " [26517.143]\n",
      " [26558.844]\n",
      " [25954.229]\n",
      " [25777.502]\n",
      " [25951.066]\n",
      " [25869.885]\n",
      " [25746.61 ]\n",
      " [25515.67 ]\n",
      " [25658.748]\n",
      " [25801.068]\n",
      " [25868.94 ]\n",
      " [25785.473]\n",
      " [25666.576]\n",
      " [25687.86 ]\n",
      " [25666.125]\n",
      " [25782.975]\n",
      " [25811.164]\n",
      " [25514.508]\n",
      " [25413.113]\n",
      " [25627.424]\n",
      " [25407.688]\n",
      " [25402.021]\n",
      " [25400.379]\n",
      " [25380.805]\n",
      " [25506.182]\n",
      " [25610.861]]\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "RMSE: 0.000330938398256279\n",
      "MAE: 0.0002776021253427656\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "veri = pd.read_csv(r\"C:/Users/ASUS/Desktop/AI CLUB DATATHON/CSV/zzc_ail.csv\")\n",
    "\n",
    "veri['open'] = veri['open'].str.replace('\"', '').astype(float)\n",
    "veri['high'] = veri['high'].str.replace('\"', '').astype(float)\n",
    "veri['low'] = veri['low'].str.replace('\"', '').astype(float)\n",
    "veri['close'] = veri['close'].str.replace('\"', '').astype(float)\n",
    "\n",
    "veri['open'].fillna(veri['open'].mean(), inplace=True)\n",
    "veri['high'].fillna(veri['high'].mean(), inplace=True)\n",
    "veri['low'].fillna(veri['low'].mean(), inplace=True)\n",
    "veri['close'].fillna(veri['close'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "veri = veri[['open', 'low', 'high', 'close']]\n",
    "\n",
    "X = veri[['open', 'high', 'low']].values\n",
    "y = veri['high'].values\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "\n",
    "model.fit(X_scaled, y_scaled, epochs=25, batch_size=32, verbose=1)\n",
    "\n",
    "\n",
    "gelecek_veri = X_scaled[-201:] \n",
    "tahminler = model.predict(gelecek_veri)\n",
    "\n",
    "\n",
    "tahminler = scaler.inverse_transform(tahminler)\n",
    "\n",
    "print(\"Gelecekteki Fiyat Tahminleri:\")\n",
    "print(tahminler)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3feae85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0185\n",
      "Epoch 2/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.2344e-06\n",
      "Epoch 3/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.7664e-06\n",
      "Epoch 4/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3211e-06\n",
      "Epoch 5/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0229e-06\n",
      "Epoch 6/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4939e-06\n",
      "Epoch 7/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2801e-06\n",
      "Epoch 8/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3525e-06\n",
      "Epoch 9/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0585e-06\n",
      "Epoch 10/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0616e-06\n",
      "Epoch 11/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2644e-06\n",
      "Epoch 12/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0310e-06\n",
      "Epoch 13/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.1882e-07\n",
      "Epoch 14/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.0542e-07\n",
      "Epoch 15/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.7388e-07\n",
      "Epoch 16/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.8575e-06\n",
      "Epoch 17/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.8119e-06\n",
      "Epoch 18/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.0710e-07\n",
      "Epoch 19/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.2924e-07\n",
      "Epoch 20/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.8676e-07\n",
      "Epoch 21/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.7266e-06\n",
      "Epoch 22/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.9850e-07\n",
      "Epoch 23/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0493e-06\n",
      "Epoch 24/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3268e-06\n",
      "Epoch 25/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.9646e-06\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Gelecekteki Fiyat Tahminleri:\n",
      "[[23852.3  ]\n",
      " [23744.932]\n",
      " [23738.182]\n",
      " [23896.307]\n",
      " [24853.064]\n",
      " [24757.57 ]\n",
      " [25148.582]\n",
      " [24977.754]\n",
      " [24979.156]\n",
      " [25093.547]\n",
      " [24524.24 ]\n",
      " [24704.115]\n",
      " [24904.135]\n",
      " [24678.873]\n",
      " [25042.2  ]\n",
      " [25219.197]\n",
      " [25221.748]\n",
      " [25215.26 ]\n",
      " [25220.307]\n",
      " [25080.385]\n",
      " [24923.906]\n",
      " [24840.404]\n",
      " [24837.154]\n",
      " [24791.48 ]\n",
      " [24913.256]\n",
      " [24885.273]\n",
      " [24814.521]\n",
      " [24829.98 ]\n",
      " [24827.701]\n",
      " [24863.86 ]\n",
      " [25076.365]\n",
      " [25153.531]\n",
      " [25195.695]\n",
      " [25140.914]\n",
      " [25180.78 ]\n",
      " [25173.85 ]\n",
      " [25170.715]\n",
      " [25234.309]\n",
      " [25355.041]\n",
      " [25432.236]\n",
      " [25506.81 ]\n",
      " [25441.064]\n",
      " [25517.6  ]\n",
      " [25366.98 ]\n",
      " [25309.502]\n",
      " [25223.88 ]\n",
      " [25276.1  ]\n",
      " [25308.258]\n",
      " [25336.234]\n",
      " [25263.42 ]\n",
      " [25255.16 ]\n",
      " [25425.701]\n",
      " [25361.271]\n",
      " [25356.592]\n",
      " [25370.578]\n",
      " [25386.643]\n",
      " [25472.42 ]\n",
      " [25385.94 ]\n",
      " [25568.303]\n",
      " [25477.12 ]\n",
      " [25021.66 ]\n",
      " [25613.8  ]\n",
      " [25770.752]\n",
      " [25800.895]\n",
      " [26039.697]\n",
      " [26076.186]\n",
      " [25750.828]\n",
      " [25805.418]\n",
      " [25390.418]\n",
      " [25568.922]\n",
      " [25748.037]\n",
      " [25798.354]\n",
      " [25880.229]\n",
      " [25859.67 ]\n",
      " [26162.637]\n",
      " [26824.83 ]\n",
      " [26889.35 ]\n",
      " [27008.766]\n",
      " [26954.225]\n",
      " [26980.77 ]\n",
      " [26966.834]\n",
      " [27033.477]\n",
      " [27185.467]\n",
      " [27136.303]\n",
      " [27270.166]\n",
      " [27278.307]\n",
      " [27155.6  ]\n",
      " [27165.457]\n",
      " [27163.21 ]\n",
      " [27356.703]\n",
      " [27219.496]\n",
      " [27188.908]\n",
      " [27160.67 ]\n",
      " [26998.777]\n",
      " [26936.592]\n",
      " [27078.424]\n",
      " [27203.605]\n",
      " [27081.412]\n",
      " [27062.139]\n",
      " [27122.324]\n",
      " [26998.562]\n",
      " [27069.516]\n",
      " [27044.459]\n",
      " [26840.61 ]\n",
      " [26466.205]\n",
      " [25986.719]\n",
      " [26473.17 ]\n",
      " [26743.309]\n",
      " [26633.576]\n",
      " [26600.562]\n",
      " [26598.799]\n",
      " [26726.592]\n",
      " [26909.178]\n",
      " [26931.576]\n",
      " [27140.646]\n",
      " [27151.105]\n",
      " [27173.822]\n",
      " [27217.637]\n",
      " [27245.576]\n",
      " [27449.127]\n",
      " [27512.898]\n",
      " [27650.084]\n",
      " [27630.16 ]\n",
      " [27600.916]\n",
      " [27401.508]\n",
      " [27111.668]\n",
      " [27226.494]\n",
      " [27320.1  ]\n",
      " [27337.36 ]\n",
      " [27481.152]\n",
      " [27537.234]\n",
      " [27571.566]\n",
      " [27563.652]\n",
      " [27522.127]\n",
      " [27543.326]\n",
      " [27515.865]\n",
      " [27416.678]\n",
      " [27530.135]\n",
      " [27603.197]\n",
      " [27497.723]\n",
      " [27514.297]\n",
      " [27638.768]\n",
      " [27649.46 ]\n",
      " [27678.531]\n",
      " [27666.834]\n",
      " [27469.65 ]\n",
      " [27385.79 ]\n",
      " [27029.871]\n",
      " [26910.807]\n",
      " [26672.645]\n",
      " [26399.113]\n",
      " [26635.459]\n",
      " [26220.137]\n",
      " [25919.23 ]\n",
      " [26121.115]\n",
      " [26529.426]\n",
      " [26839.955]\n",
      " [26967.764]\n",
      " [26900.709]\n",
      " [27046.533]\n",
      " [25736.566]\n",
      " [25252.803]\n",
      " [25282.57 ]\n",
      " [25343.936]\n",
      " [25739.049]\n",
      " [25326.615]\n",
      " [24790.73 ]\n",
      " [25394.   ]\n",
      " [25204.459]\n",
      " [25432.205]\n",
      " [25464.102]\n",
      " [25639.758]\n",
      " [25697.875]\n",
      " [25512.102]\n",
      " [25501.35 ]\n",
      " [25727.537]\n",
      " [25926.783]\n",
      " [25514.77 ]\n",
      " [25723.678]\n",
      " [25768.541]\n",
      " [25742.22 ]\n",
      " [25423.459]\n",
      " [25228.459]\n",
      " [25581.133]\n",
      " [25558.52 ]\n",
      " [25837.69 ]\n",
      " [25732.902]\n",
      " [25433.098]\n",
      " [25550.713]\n",
      " [25566.084]\n",
      " [25673.744]\n",
      " [25397.979]\n",
      " [25186.219]\n",
      " [25166.088]\n",
      " [25304.727]\n",
      " [25153.596]\n",
      " [25299.186]\n",
      " [25263.076]\n",
      " [25293.35 ]\n",
      " [25420.727]\n",
      " [25513.229]]\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "RMSE: 0.0011752951749601454\n",
      "MAE: 0.0008784334568683738\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "veri = pd.read_csv(r\"C:/Users/ASUS/Desktop/AI CLUB DATATHON/CSV/zzc_ail.csv\")\n",
    "\n",
    "veri['open'] = veri['open'].str.replace('\"', '').astype(float)\n",
    "veri['high'] = veri['high'].str.replace('\"', '').astype(float)\n",
    "veri['low'] = veri['low'].str.replace('\"', '').astype(float)\n",
    "veri['close'] = veri['close'].str.replace('\"', '').astype(float)\n",
    "\n",
    "veri['open'].fillna(veri['open'].mean(), inplace=True)\n",
    "veri['high'].fillna(veri['high'].mean(), inplace=True)\n",
    "veri['low'].fillna(veri['low'].mean(), inplace=True)\n",
    "veri['close'].fillna(veri['close'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "veri = veri[['open', 'low', 'high', 'close']]\n",
    "\n",
    "X = veri[['open', 'high', 'low']].values\n",
    "y = veri['low'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_scaled, y_scaled, epochs=25, batch_size=32, verbose=1)\n",
    "\n",
    "gelecek_veri = X_scaled[-201:]  \n",
    "tahminler = model.predict(gelecek_veri)\n",
    "\n",
    "\n",
    "tahminler = scaler.inverse_transform(tahminler)\n",
    "\n",
    "print(\"Gelecekteki Fiyat Tahminleri:\")\n",
    "print(tahminler)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cecd012c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0055\n",
      "Epoch 2/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0021\n",
      "Epoch 3/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018\n",
      "Epoch 4/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0020\n",
      "Epoch 5/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0021\n",
      "Epoch 6/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0020\n",
      "Epoch 7/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0021\n",
      "Epoch 8/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0019\n",
      "Epoch 9/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0022\n",
      "Epoch 10/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018\n",
      "Epoch 11/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0017\n",
      "Epoch 12/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0020\n",
      "Epoch 13/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0022\n",
      "Epoch 14/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0019\n",
      "Epoch 15/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0017\n",
      "Epoch 16/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0021\n",
      "Epoch 17/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0021\n",
      "Epoch 18/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0020\n",
      "Epoch 19/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0023\n",
      "Epoch 20/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0023\n",
      "Epoch 21/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0018\n",
      "Epoch 22/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0023\n",
      "Epoch 23/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0019\n",
      "Epoch 24/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018\n",
      "Epoch 25/25\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0020\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Gelecekteki Fiyat Tahminleri:\n",
      "[[22398.5  ]\n",
      " [22296.781]\n",
      " [22259.416]\n",
      " [22454.293]\n",
      " [23343.756]\n",
      " [23243.453]\n",
      " [23577.275]\n",
      " [23502.238]\n",
      " [23387.04 ]\n",
      " [23531.291]\n",
      " [23329.557]\n",
      " [23227.174]\n",
      " [23403.05 ]\n",
      " [23218.203]\n",
      " [23480.533]\n",
      " [23641.809]\n",
      " [23674.71 ]\n",
      " [23639.84 ]\n",
      " [23615.111]\n",
      " [23540.223]\n",
      " [23376.188]\n",
      " [23270.998]\n",
      " [23281.873]\n",
      " [23284.605]\n",
      " [23369.55 ]\n",
      " [23319.342]\n",
      " [23245.57 ]\n",
      " [23249.979]\n",
      " [23251.07 ]\n",
      " [23289.248]\n",
      " [23479.434]\n",
      " [23536.045]\n",
      " [23566.945]\n",
      " [23577.33 ]\n",
      " [23587.682]\n",
      " [23560.318]\n",
      " [23636.326]\n",
      " [23658.148]\n",
      " [23733.852]\n",
      " [23810.648]\n",
      " [23860.564]\n",
      " [23834.748]\n",
      " [23869.453]\n",
      " [23784.695]\n",
      " [23706.098]\n",
      " [23603.559]\n",
      " [23645.254]\n",
      " [23681.838]\n",
      " [23700.053]\n",
      " [23649.645]\n",
      " [23656.654]\n",
      " [23811.914]\n",
      " [23739.418]\n",
      " [23723.646]\n",
      " [23722.172]\n",
      " [24595.209]\n",
      " [23856.977]\n",
      " [24619.748]\n",
      " [23966.97 ]\n",
      " [24068.727]\n",
      " [23638.729]\n",
      " [24003.91 ]\n",
      " [24126.848]\n",
      " [24169.371]\n",
      " [24352.326]\n",
      " [24377.621]\n",
      " [24177.627]\n",
      " [24138.02 ]\n",
      " [24000.844]\n",
      " [23970.684]\n",
      " [24984.43 ]\n",
      " [24115.607]\n",
      " [24187.77 ]\n",
      " [24213.21 ]\n",
      " [24561.465]\n",
      " [25061.975]\n",
      " [25123.455]\n",
      " [25235.727]\n",
      " [25213.75 ]\n",
      " [25225.018]\n",
      " [25272.158]\n",
      " [25265.979]\n",
      " [25421.371]\n",
      " [25375.94 ]\n",
      " [25487.58 ]\n",
      " [25491.195]\n",
      " [25444.1  ]\n",
      " [25364.791]\n",
      " [25386.824]\n",
      " [25535.37 ]\n",
      " [25435.213]\n",
      " [25385.527]\n",
      " [25372.328]\n",
      " [25297.115]\n",
      " [25197.287]\n",
      " [25286.295]\n",
      " [25410.457]\n",
      " [25281.01 ]\n",
      " [25287.955]\n",
      " [25323.363]\n",
      " [25254.576]\n",
      " [25309.654]\n",
      " [25342.129]\n",
      " [25127.793]\n",
      " [24902.135]\n",
      " [24504.094]\n",
      " [25857.479]\n",
      " [24988.135]\n",
      " [24923.488]\n",
      " [24860.79 ]\n",
      " [24853.533]\n",
      " [24970.82 ]\n",
      " [25127.691]\n",
      " [25181.5  ]\n",
      " [25338.672]\n",
      " [25343.715]\n",
      " [25367.117]\n",
      " [25404.404]\n",
      " [25449.229]\n",
      " [25631.543]\n",
      " [25710.16 ]\n",
      " [25825.346]\n",
      " [25787.97 ]\n",
      " [25771.336]\n",
      " [25642.053]\n",
      " [25468.703]\n",
      " [25456.299]\n",
      " [25533.506]\n",
      " [25563.82 ]\n",
      " [25678.611]\n",
      " [25733.629]\n",
      " [25775.719]\n",
      " [25728.486]\n",
      " [25728.182]\n",
      " [25695.889]\n",
      " [25699.512]\n",
      " [25611.975]\n",
      " [25693.064]\n",
      " [25744.053]\n",
      " [25738.83 ]\n",
      " [25686.578]\n",
      " [25794.088]\n",
      " [25851.297]\n",
      " [25816.326]\n",
      " [25816.658]\n",
      " [25655.652]\n",
      " [25609.29 ]\n",
      " [25422.734]\n",
      " [25241.873]\n",
      " [25076.246]\n",
      " [24817.256]\n",
      " [26087.295]\n",
      " [24726.951]\n",
      " [24453.3  ]\n",
      " [24523.06 ]\n",
      " [24842.635]\n",
      " [25068.916]\n",
      " [25179.574]\n",
      " [25183.36 ]\n",
      " [25274.545]\n",
      " [24582.74 ]\n",
      " [23964.86 ]\n",
      " [23837.309]\n",
      " [23774.064]\n",
      " [24122.44 ]\n",
      " [23883.639]\n",
      " [23456.344]\n",
      " [23821.264]\n",
      " [23666.46 ]\n",
      " [24673.162]\n",
      " [23904.941]\n",
      " [24047.078]\n",
      " [25003.928]\n",
      " [23940.768]\n",
      " [24750.49 ]\n",
      " [24173.76 ]\n",
      " [24478.002]\n",
      " [24057.836]\n",
      " [24039.863]\n",
      " [24103.256]\n",
      " [24080.95 ]\n",
      " [23922.79 ]\n",
      " [23676.326]\n",
      " [23916.38 ]\n",
      " [23948.422]\n",
      " [24167.537]\n",
      " [24082.541]\n",
      " [23891.357]\n",
      " [23931.99 ]\n",
      " [23917.955]\n",
      " [24014.52 ]\n",
      " [23932.953]\n",
      " [23684.727]\n",
      " [23600.287]\n",
      " [23753.803]\n",
      " [23609.873]\n",
      " [23704.23 ]\n",
      " [23648.152]\n",
      " [23655.424]\n",
      " [23800.592]\n",
      " [23872.613]]\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "RMSE: 0.05041986269688317\n",
      "MAE: 0.023680437711474354\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "veri = pd.read_csv(r\"C:/Users/ASUS/Desktop/AI CLUB DATATHON/CSV/zzc_ail.csv\")\n",
    "\n",
    "veri['open'] = veri['open'].str.replace('\"', '').astype(float)\n",
    "veri['high'] = veri['high'].str.replace('\"', '').astype(float)\n",
    "veri['low'] = veri['low'].str.replace('\"', '').astype(float)\n",
    "veri['close'] = veri['close'].str.replace('\"', '').astype(float)\n",
    "\n",
    "veri['open'].fillna(veri['open'].mean(), inplace=True)\n",
    "veri['high'].fillna(veri['high'].mean(), inplace=True)\n",
    "veri['low'].fillna(veri['low'].mean(), inplace=True)\n",
    "veri['close'].fillna(veri['close'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "veri = veri[['open', 'low', 'high', 'close']]\n",
    "\n",
    "X = veri[['open', 'high', 'low']].values\n",
    "y = veri['close'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_scaled, y_scaled, epochs=25, batch_size=32, verbose=1)\n",
    "\n",
    "gelecek_veri = X_scaled[-201:]  # Tüm verilerin son 89 verisini kullanıyoruz\n",
    "tahminler = model.predict(gelecek_veri)\n",
    "\n",
    "tahminler = scaler.inverse_transform(tahminler)\n",
    "\n",
    "print(\"Gelecekteki Fiyat Tahminleri:\")\n",
    "print(tahminler)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a9b63f",
   "metadata": {},
   "source": [
    "**ail_frx kısmı bu kodlar aracılığı ile yapıldı.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "171164b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0314\n",
      "Epoch 2/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.5997e-06\n",
      "Epoch 3/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.7128e-06\n",
      "Epoch 4/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.9581e-06\n",
      "Epoch 5/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0711e-06\n",
      "Epoch 6/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1813e-06\n",
      "Epoch 7/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1230e-06\n",
      "Epoch 8/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.9547e-06\n",
      "Epoch 9/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2674e-06\n",
      "Epoch 10/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5260e-06\n",
      "Epoch 11/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.4798e-06\n",
      "Epoch 12/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.6496e-06\n",
      "Epoch 13/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.5276e-06\n",
      "Epoch 14/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1326e-06\n",
      "Epoch 15/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3528e-06\n",
      "Epoch 16/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2273e-06\n",
      "Epoch 17/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.7572e-06\n",
      "Epoch 18/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0166e-06\n",
      "Epoch 19/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.8404e-06\n",
      "Epoch 20/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.9821e-06\n",
      "Epoch 21/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.4832e-06\n",
      "Epoch 22/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.6092e-06\n",
      "Epoch 23/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.9422e-06\n",
      "Epoch 24/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.7988e-06\n",
      "Epoch 25/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.2469e-06\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Gelecekteki Fiyat Tahminleri:\n",
      "[[14.265686 ]\n",
      " [14.264134 ]\n",
      " [14.263401 ]\n",
      " [14.283654 ]\n",
      " [14.2631035]\n",
      " [14.2626915]\n",
      " [14.261282 ]\n",
      " [14.262968 ]\n",
      " [14.309646 ]\n",
      " [14.266759 ]\n",
      " [14.267619 ]\n",
      " [14.269239 ]\n",
      " [14.272859 ]\n",
      " [14.273415 ]\n",
      " [14.278101 ]\n",
      " [14.280927 ]\n",
      " [14.282512 ]\n",
      " [14.281318 ]\n",
      " [14.282728 ]\n",
      " [14.286574 ]\n",
      " [14.287751 ]\n",
      " [14.280328 ]\n",
      " [14.296904 ]\n",
      " [14.300331 ]\n",
      " [14.299552 ]\n",
      " [14.299396 ]\n",
      " [14.305658 ]\n",
      " [14.311224 ]\n",
      " [14.314793 ]\n",
      " [14.3160515]\n",
      " [14.318283 ]\n",
      " [14.318148 ]\n",
      " [14.320589 ]\n",
      " [14.320487 ]\n",
      " [14.31974  ]\n",
      " [14.318864 ]\n",
      " [14.317104 ]\n",
      " [14.319963 ]\n",
      " [14.31289  ]\n",
      " [14.311619 ]\n",
      " [14.316535 ]\n",
      " [14.312172 ]\n",
      " [14.318509 ]\n",
      " [14.339562 ]\n",
      " [14.361766 ]\n",
      " [14.362633 ]\n",
      " [14.363131 ]\n",
      " [14.365618 ]\n",
      " [14.36849  ]\n",
      " [14.367969 ]\n",
      " [14.364935 ]\n",
      " [14.368728 ]\n",
      " [14.368736 ]\n",
      " [14.368006 ]\n",
      " [14.377705 ]\n",
      " [14.371225 ]\n",
      " [14.373147 ]\n",
      " [14.373659 ]\n",
      " [14.375245 ]\n",
      " [14.377011 ]\n",
      " [14.361865 ]\n",
      " [14.359163 ]\n",
      " [14.351724 ]\n",
      " [14.357038 ]\n",
      " [14.380824 ]\n",
      " [14.391841 ]\n",
      " [14.393525 ]\n",
      " [14.393555 ]\n",
      " [14.391149 ]\n",
      " [14.390387 ]\n",
      " [14.389868 ]\n",
      " [14.389391 ]\n",
      " [14.39129  ]\n",
      " [14.389207 ]\n",
      " [14.395382 ]\n",
      " [14.390063 ]\n",
      " [14.389803 ]\n",
      " [14.38999  ]\n",
      " [14.389649 ]\n",
      " [14.391044 ]\n",
      " [14.416995 ]\n",
      " [14.39312  ]\n",
      " [14.394411 ]\n",
      " [14.396838 ]\n",
      " [14.392902 ]\n",
      " [14.393635 ]\n",
      " [14.396023 ]\n",
      " [14.404543 ]\n",
      " [14.4111395]\n",
      " [14.41001  ]\n",
      " [14.408494 ]\n",
      " [14.4072485]\n",
      " [14.40841  ]\n",
      " [14.409466 ]\n",
      " [14.409305 ]\n",
      " [14.4084015]\n",
      " [14.408863 ]\n",
      " [14.409138 ]\n",
      " [14.409238 ]\n",
      " [14.409391 ]\n",
      " [14.409852 ]\n",
      " [14.4096365]\n",
      " [14.411124 ]\n",
      " [14.413535 ]\n",
      " [14.414159 ]\n",
      " [14.418303 ]\n",
      " [14.415914 ]\n",
      " [14.483509 ]\n",
      " [14.418657 ]\n",
      " [14.409276 ]\n",
      " [14.416183 ]\n",
      " [14.445926 ]\n",
      " [14.435576 ]\n",
      " [14.458225 ]\n",
      " [14.495775 ]\n",
      " [14.45127  ]\n",
      " [14.496017 ]\n",
      " [14.446349 ]\n",
      " [14.429961 ]\n",
      " [14.431455 ]\n",
      " [14.430305 ]\n",
      " [14.431639 ]\n",
      " [14.431279 ]\n",
      " [14.431582 ]\n",
      " [14.431098 ]\n",
      " [14.432488 ]\n",
      " [14.433274 ]\n",
      " [14.43289  ]\n",
      " [14.432374 ]\n",
      " [14.432464 ]\n",
      " [14.429837 ]\n",
      " [14.42838  ]\n",
      " [14.427289 ]\n",
      " [14.42974  ]\n",
      " [14.438323 ]\n",
      " [14.453854 ]\n",
      " [14.445852 ]\n",
      " [14.446012 ]\n",
      " [14.4451275]\n",
      " [14.4458275]\n",
      " [14.445273 ]\n",
      " [14.444143 ]\n",
      " [14.445065 ]\n",
      " [14.446023 ]\n",
      " [14.445467 ]\n",
      " [14.446286 ]\n",
      " [14.4453745]\n",
      " [14.444784 ]\n",
      " [14.445197 ]\n",
      " [14.442513 ]\n",
      " [14.444031 ]\n",
      " [14.446415 ]\n",
      " [14.446733 ]\n",
      " [14.45224  ]\n",
      " [14.461165 ]\n",
      " [14.472392 ]\n",
      " [14.480552 ]\n",
      " [14.479935 ]\n",
      " [14.54758  ]\n",
      " [14.57854  ]\n",
      " [14.521246 ]\n",
      " [14.490207 ]\n",
      " [14.536542 ]\n",
      " [14.484169 ]\n",
      " [14.48731  ]\n",
      " [14.484205 ]\n",
      " [14.484649 ]\n",
      " [14.48595  ]\n",
      " [14.48663  ]\n",
      " [14.48737  ]\n",
      " [14.487092 ]\n",
      " [14.485684 ]\n",
      " [14.485431 ]\n",
      " [14.483172 ]\n",
      " [14.472586 ]\n",
      " [14.466604 ]\n",
      " [14.456683 ]\n",
      " [14.477485 ]\n",
      " [14.506206 ]\n",
      " [14.527969 ]\n",
      " [14.526661 ]\n",
      " [14.527497 ]\n",
      " [14.52143  ]\n",
      " [14.542024 ]\n",
      " [14.523696 ]\n",
      " [14.523769 ]\n",
      " [14.523716 ]\n",
      " [14.523457 ]\n",
      " [14.524108 ]\n",
      " [14.524186 ]\n",
      " [14.524357 ]\n",
      " [14.525089 ]\n",
      " [14.524102 ]\n",
      " [14.523304 ]\n",
      " [14.519765 ]\n",
      " [14.524507 ]\n",
      " [14.523908 ]\n",
      " [14.535981 ]\n",
      " [14.535927 ]\n",
      " [14.540493 ]\n",
      " [14.541479 ]]\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "RMSE: 0.0011945772846795094\n",
      "MAE: 0.0007662062744950194\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "veri = pd.read_csv(r\"C:/Users/ASUS/Desktop/AI CLUB DATATHON/CSV/ail_frx.csv\")\n",
    "\n",
    "veri['open'] = veri['open'].str.replace('\"', '').astype(float)\n",
    "veri['high'] = veri['high'].str.replace('\"', '').astype(float)\n",
    "veri['low'] = veri['low'].str.replace('\"', '').astype(float)\n",
    "veri['close'] = veri['close'].str.replace('\"', '').astype(float)\n",
    "\n",
    "veri['open'].fillna(veri['open'].mean(), inplace=True)\n",
    "veri['high'].fillna(veri['high'].mean(), inplace=True)\n",
    "veri['low'].fillna(veri['low'].mean(), inplace=True)\n",
    "veri['close'].fillna(veri['close'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "veri = veri[['open', 'low', 'high', 'close']]\n",
    "\n",
    "X = veri[['close', 'open', 'high']].values\n",
    "y = veri['open'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_scaled, y_scaled, epochs=25, batch_size=32, verbose=1)\n",
    "\n",
    "gelecek_veri = X_scaled[-201:]  \n",
    "tahminler = model.predict(gelecek_veri)\n",
    "\n",
    "tahminler = scaler.inverse_transform(tahminler)\n",
    "\n",
    "print(\"Gelecekteki Fiyat Tahminleri:\")\n",
    "print(tahminler)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9bffa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0096\n",
      "Epoch 2/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3635e-06\n",
      "Epoch 3/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4124e-06\n",
      "Epoch 4/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0876e-06\n",
      "Epoch 5/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3421e-06\n",
      "Epoch 6/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2658e-06\n",
      "Epoch 7/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.4336e-07\n",
      "Epoch 8/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.8987e-07\n",
      "Epoch 9/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.6911e-07\n",
      "Epoch 10/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.3682e-07\n",
      "Epoch 11/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.5779e-07\n",
      "Epoch 12/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.0393e-07\n",
      "Epoch 13/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.4482e-07\n",
      "Epoch 14/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.2942e-07\n",
      "Epoch 15/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.0371e-07\n",
      "Epoch 16/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.2690e-07\n",
      "Epoch 17/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0376e-06\n",
      "Epoch 18/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.8039e-06\n",
      "Epoch 19/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1519e-06\n",
      "Epoch 20/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5819e-06\n",
      "Epoch 21/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.7386e-06\n",
      "Epoch 22/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0832e-06\n",
      "Epoch 23/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.2494e-06\n",
      "Epoch 24/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.6845e-07\n",
      "Epoch 25/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4131e-06\n",
      "Epoch 26/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4671e-06\n",
      "Epoch 27/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2906e-06\n",
      "Epoch 28/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.4586e-07\n",
      "Epoch 29/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.0270e-07\n",
      "Epoch 30/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2781e-06\n",
      "Epoch 31/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.4542e-07\n",
      "Epoch 32/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1125e-06\n",
      "Epoch 33/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2365e-06\n",
      "Epoch 34/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2970e-07\n",
      "Epoch 35/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1297e-06\n",
      "Epoch 36/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3426e-06\n",
      "Epoch 37/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.7339e-07\n",
      "Epoch 38/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4670e-06\n",
      "Epoch 39/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.9501e-07\n",
      "Epoch 40/40\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.3417e-06\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "Gelecekteki Fiyat Tahminleri:\n",
      "[[14.37092  ]\n",
      " [14.36142  ]\n",
      " [14.363637 ]\n",
      " [14.398416 ]\n",
      " [14.356109 ]\n",
      " [14.352097 ]\n",
      " [14.351755 ]\n",
      " [14.352729 ]\n",
      " [14.477439 ]\n",
      " [14.356011 ]\n",
      " [14.356479 ]\n",
      " [14.362084 ]\n",
      " [14.362558 ]\n",
      " [14.365112 ]\n",
      " [14.368308 ]\n",
      " [14.370106 ]\n",
      " [14.370973 ]\n",
      " [14.370783 ]\n",
      " [14.371469 ]\n",
      " [14.377347 ]\n",
      " [14.379084 ]\n",
      " [14.387599 ]\n",
      " [14.388406 ]\n",
      " [14.388657 ]\n",
      " [14.389968 ]\n",
      " [14.391331 ]\n",
      " [14.400975 ]\n",
      " [14.40249  ]\n",
      " [14.408512 ]\n",
      " [14.40699  ]\n",
      " [14.407174 ]\n",
      " [14.408674 ]\n",
      " [14.409769 ]\n",
      " [14.41008  ]\n",
      " [14.409033 ]\n",
      " [14.408136 ]\n",
      " [14.410127 ]\n",
      " [14.409014 ]\n",
      " [14.403575 ]\n",
      " [14.400427 ]\n",
      " [14.414823 ]\n",
      " [14.404262 ]\n",
      " [14.419423 ]\n",
      " [14.452425 ]\n",
      " [14.455838 ]\n",
      " [14.452818 ]\n",
      " [14.453421 ]\n",
      " [14.455773 ]\n",
      " [14.457028 ]\n",
      " [14.457166 ]\n",
      " [14.45724  ]\n",
      " [14.458043 ]\n",
      " [14.458874 ]\n",
      " [14.457919 ]\n",
      " [14.484898 ]\n",
      " [14.4625225]\n",
      " [14.464486 ]\n",
      " [14.463327 ]\n",
      " [14.465419 ]\n",
      " [14.466299 ]\n",
      " [14.451173 ]\n",
      " [14.44933  ]\n",
      " [14.450191 ]\n",
      " [14.452917 ]\n",
      " [14.484275 ]\n",
      " [14.482959 ]\n",
      " [14.485825 ]\n",
      " [14.486707 ]\n",
      " [14.48096  ]\n",
      " [14.479844 ]\n",
      " [14.479516 ]\n",
      " [14.479736 ]\n",
      " [14.484439 ]\n",
      " [14.479234 ]\n",
      " [14.493387 ]\n",
      " [14.479494 ]\n",
      " [14.479502 ]\n",
      " [14.479248 ]\n",
      " [14.479599 ]\n",
      " [14.483168 ]\n",
      " [14.546634 ]\n",
      " [14.482888 ]\n",
      " [14.485623 ]\n",
      " [14.486433 ]\n",
      " [14.4826145]\n",
      " [14.482985 ]\n",
      " [14.489262 ]\n",
      " [14.504268 ]\n",
      " [14.502048 ]\n",
      " [14.498484 ]\n",
      " [14.498242 ]\n",
      " [14.497439 ]\n",
      " [14.497676 ]\n",
      " [14.498901 ]\n",
      " [14.499404 ]\n",
      " [14.497835 ]\n",
      " [14.498845 ]\n",
      " [14.499626 ]\n",
      " [14.499127 ]\n",
      " [14.500405 ]\n",
      " [14.499769 ]\n",
      " [14.499552 ]\n",
      " [14.502291 ]\n",
      " [14.502725 ]\n",
      " [14.507642 ]\n",
      " [14.507665 ]\n",
      " [14.506074 ]\n",
      " [14.678766 ]\n",
      " [14.508283 ]\n",
      " [14.506937 ]\n",
      " [14.511333 ]\n",
      " [14.602653 ]\n",
      " [14.534112 ]\n",
      " [14.57933  ]\n",
      " [14.587334 ]\n",
      " [14.575702 ]\n",
      " [14.653853 ]\n",
      " [14.567198 ]\n",
      " [14.520914 ]\n",
      " [14.522655 ]\n",
      " [14.520396 ]\n",
      " [14.52156  ]\n",
      " [14.521769 ]\n",
      " [14.521181 ]\n",
      " [14.521222 ]\n",
      " [14.524241 ]\n",
      " [14.523802 ]\n",
      " [14.522474 ]\n",
      " [14.522286 ]\n",
      " [14.521803 ]\n",
      " [14.521521 ]\n",
      " [14.51876  ]\n",
      " [14.518377 ]\n",
      " [14.530593 ]\n",
      " [14.53433  ]\n",
      " [14.5607395]\n",
      " [14.535752 ]\n",
      " [14.536734 ]\n",
      " [14.535107 ]\n",
      " [14.535716 ]\n",
      " [14.53538  ]\n",
      " [14.534724 ]\n",
      " [14.536947 ]\n",
      " [14.5357065]\n",
      " [14.535623 ]\n",
      " [14.53747  ]\n",
      " [14.534415 ]\n",
      " [14.535145 ]\n",
      " [14.535058 ]\n",
      " [14.533175 ]\n",
      " [14.533653 ]\n",
      " [14.538173 ]\n",
      " [14.537494 ]\n",
      " [14.551138 ]\n",
      " [14.580138 ]\n",
      " [14.57268  ]\n",
      " [14.572558 ]\n",
      " [14.5722065]\n",
      " [14.748278 ]\n",
      " [14.776507 ]\n",
      " [14.678849 ]\n",
      " [14.584242 ]\n",
      " [14.707205 ]\n",
      " [14.576754 ]\n",
      " [14.583414 ]\n",
      " [14.575291 ]\n",
      " [14.575097 ]\n",
      " [14.577669 ]\n",
      " [14.577321 ]\n",
      " [14.579139 ]\n",
      " [14.57815  ]\n",
      " [14.576984 ]\n",
      " [14.575767 ]\n",
      " [14.573488 ]\n",
      " [14.565437 ]\n",
      " [14.561133 ]\n",
      " [14.587017 ]\n",
      " [14.594309 ]\n",
      " [14.620886 ]\n",
      " [14.61996  ]\n",
      " [14.620692 ]\n",
      " [14.618488 ]\n",
      " [14.617715 ]\n",
      " [14.666649 ]\n",
      " [14.61486  ]\n",
      " [14.614808 ]\n",
      " [14.615795 ]\n",
      " [14.614849 ]\n",
      " [14.6158905]\n",
      " [14.61534  ]\n",
      " [14.615852 ]\n",
      " [14.616315 ]\n",
      " [14.614916 ]\n",
      " [14.616552 ]\n",
      " [14.615568 ]\n",
      " [14.615507 ]\n",
      " [14.615392 ]\n",
      " [14.645705 ]\n",
      " [14.654919 ]\n",
      " [14.637547 ]\n",
      " [14.633227 ]]\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step\n",
      "RMSE: 0.007185994284786264\n",
      "MAE: 0.006429869016247972\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "veri = pd.read_csv(r\"C:/Users/ASUS/Desktop/AI CLUB DATATHON/CSV/ail_frx.csv\")\n",
    "\n",
    "veri['open'] = veri['open'].str.replace('\"', '').astype(float)\n",
    "veri['high'] = veri['high'].str.replace('\"', '').astype(float)\n",
    "veri['low'] = veri['low'].str.replace('\"', '').astype(float)\n",
    "veri['close'] = veri['close'].str.replace('\"', '').astype(float)\n",
    "\n",
    "veri['open'].fillna(veri['open'].mean(), inplace=True)\n",
    "veri['high'].fillna(veri['high'].mean(), inplace=True)\n",
    "veri['low'].fillna(veri['low'].mean(), inplace=True)\n",
    "veri['close'].fillna(veri['close'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "veri = veri[['open', 'low', 'high', 'close']]\n",
    "\n",
    "X = veri[['open', 'low', 'high']].values\n",
    "y = veri['high'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_scaled, y_scaled, epochs=40, batch_size=32, verbose=1)\n",
    "\n",
    "gelecek_veri = X_scaled[-201:]  \n",
    "tahminler = model.predict(gelecek_veri)\n",
    "\n",
    "\n",
    "tahminler = scaler.inverse_transform(tahminler)\n",
    "\n",
    "print(\"Gelecekteki Fiyat Tahminleri:\")\n",
    "print(tahminler)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac91c795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0234\n",
      "Epoch 2/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.4186e-06\n",
      "Epoch 3/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.2204e-06\n",
      "Epoch 4/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.4341e-06\n",
      "Epoch 5/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.9352e-06\n",
      "Epoch 6/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.1289e-06\n",
      "Epoch 7/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.6905e-06\n",
      "Epoch 8/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.2243e-06\n",
      "Epoch 9/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.0264e-06\n",
      "Epoch 10/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.4109e-06\n",
      "Epoch 11/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.2272e-06\n",
      "Epoch 12/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.9459e-06\n",
      "Epoch 13/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.7903e-06\n",
      "Epoch 14/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.1161e-06\n",
      "Epoch 15/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.8001e-06\n",
      "Epoch 16/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0129e-05\n",
      "Epoch 17/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.5468e-06\n",
      "Epoch 18/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.7683e-06\n",
      "Epoch 19/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.2865e-06\n",
      "Epoch 20/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.9681e-06\n",
      "Epoch 21/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.0829e-06\n",
      "Epoch 22/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.2659e-06\n",
      "Epoch 23/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.6202e-06\n",
      "Epoch 24/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.0496e-06\n",
      "Epoch 25/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.1247e-06\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Gelecekteki Fiyat Tahminleri:\n",
      "[[14.245492 ]\n",
      " [14.244752 ]\n",
      " [14.244617 ]\n",
      " [14.260704 ]\n",
      " [14.244955 ]\n",
      " [14.244984 ]\n",
      " [14.243921 ]\n",
      " [14.245583 ]\n",
      " [14.277243 ]\n",
      " [14.249397 ]\n",
      " [14.250165 ]\n",
      " [14.251717 ]\n",
      " [14.255325 ]\n",
      " [14.256014 ]\n",
      " [14.260753 ]\n",
      " [14.263605 ]\n",
      " [14.264639 ]\n",
      " [14.263676 ]\n",
      " [14.265212 ]\n",
      " [14.221781 ]\n",
      " [14.267796 ]\n",
      " [14.2625   ]\n",
      " [14.279585 ]\n",
      " [14.282902 ]\n",
      " [14.281944 ]\n",
      " [14.282043 ]\n",
      " [14.288142 ]\n",
      " [14.293682 ]\n",
      " [14.296921 ]\n",
      " [14.298638 ]\n",
      " [14.300764 ]\n",
      " [14.30082  ]\n",
      " [14.303091 ]\n",
      " [14.302999 ]\n",
      " [14.302261 ]\n",
      " [14.301406 ]\n",
      " [14.2520895]\n",
      " [14.301395 ]\n",
      " [14.29517  ]\n",
      " [14.294153 ]\n",
      " [14.297182 ]\n",
      " [14.293831 ]\n",
      " [14.300293 ]\n",
      " [14.321107 ]\n",
      " [14.343628 ]\n",
      " [14.29673  ]\n",
      " [14.297234 ]\n",
      " [14.348319 ]\n",
      " [14.302365 ]\n",
      " [14.350209 ]\n",
      " [14.347521 ]\n",
      " [14.351366 ]\n",
      " [14.35123  ]\n",
      " [14.350514 ]\n",
      " [14.357326 ]\n",
      " [14.35381  ]\n",
      " [14.355614 ]\n",
      " [14.307557 ]\n",
      " [14.357907 ]\n",
      " [14.357887 ]\n",
      " [14.343611 ]\n",
      " [14.339692 ]\n",
      " [14.33396  ]\n",
      " [14.33783  ]\n",
      " [14.362902 ]\n",
      " [14.374081 ]\n",
      " [14.375165 ]\n",
      " [14.374935 ]\n",
      " [14.373369 ]\n",
      " [14.372967 ]\n",
      " [14.372306 ]\n",
      " [14.371986 ]\n",
      " [14.373256 ]\n",
      " [14.371862 ]\n",
      " [14.376369 ]\n",
      " [14.372649 ]\n",
      " [14.3723755]\n",
      " [14.372553 ]\n",
      " [14.372229 ]\n",
      " [14.373706 ]\n",
      " [14.391855 ]\n",
      " [14.37568  ]\n",
      " [14.377093 ]\n",
      " [14.378947 ]\n",
      " [14.3754635]\n",
      " [14.326288 ]\n",
      " [14.378501 ]\n",
      " [14.385924 ]\n",
      " [14.393429 ]\n",
      " [14.3408375]\n",
      " [14.390923 ]\n",
      " [14.38987  ]\n",
      " [14.391066 ]\n",
      " [14.340555 ]\n",
      " [14.391903 ]\n",
      " [14.391075 ]\n",
      " [14.391309 ]\n",
      " [14.340282 ]\n",
      " [14.391819 ]\n",
      " [14.391965 ]\n",
      " [14.392368 ]\n",
      " [14.392275 ]\n",
      " [14.393826 ]\n",
      " [14.396051 ]\n",
      " [14.396786 ]\n",
      " [14.400792 ]\n",
      " [14.398482 ]\n",
      " [14.443202 ]\n",
      " [14.400525 ]\n",
      " [14.391491 ]\n",
      " [14.397818 ]\n",
      " [14.418255 ]\n",
      " [14.417663 ]\n",
      " [14.441353 ]\n",
      " [14.471614 ]\n",
      " [14.4303   ]\n",
      " [14.462458 ]\n",
      " [14.423309 ]\n",
      " [14.412408 ]\n",
      " [14.413685 ]\n",
      " [14.412933 ]\n",
      " [14.414119 ]\n",
      " [14.413848 ]\n",
      " [14.414157 ]\n",
      " [14.413817 ]\n",
      " [14.415073 ]\n",
      " [14.415735 ]\n",
      " [14.415449 ]\n",
      " [14.414965 ]\n",
      " [14.414712 ]\n",
      " [14.411559 ]\n",
      " [14.410741 ]\n",
      " [14.40756  ]\n",
      " [14.359483 ]\n",
      " [14.4208355]\n",
      " [14.433473 ]\n",
      " [14.428318 ]\n",
      " [14.428353 ]\n",
      " [14.427469 ]\n",
      " [14.428238 ]\n",
      " [14.427742 ]\n",
      " [14.426618 ]\n",
      " [14.370765 ]\n",
      " [14.428617 ]\n",
      " [14.428111 ]\n",
      " [14.428742 ]\n",
      " [14.370719 ]\n",
      " [14.427348 ]\n",
      " [14.427864 ]\n",
      " [14.425084 ]\n",
      " [14.426619 ]\n",
      " [14.372066 ]\n",
      " [14.42909  ]\n",
      " [14.433265 ]\n",
      " [14.442803 ]\n",
      " [14.454555 ]\n",
      " [14.46312  ]\n",
      " [14.401499 ]\n",
      " [14.531583 ]\n",
      " [14.47449  ]\n",
      " [14.49105  ]\n",
      " [14.472423 ]\n",
      " [14.502382 ]\n",
      " [14.466605 ]\n",
      " [14.468961 ]\n",
      " [14.46705  ]\n",
      " [14.46751  ]\n",
      " [14.468765 ]\n",
      " [14.469529 ]\n",
      " [14.470069 ]\n",
      " [14.46971  ]\n",
      " [14.468418 ]\n",
      " [14.466985 ]\n",
      " [14.4640045]\n",
      " [14.454479 ]\n",
      " [14.444294 ]\n",
      " [14.433194 ]\n",
      " [14.459487 ]\n",
      " [14.4884815]\n",
      " [14.510889 ]\n",
      " [14.509279 ]\n",
      " [14.510408 ]\n",
      " [14.440607 ]\n",
      " [14.518515 ]\n",
      " [14.506907 ]\n",
      " [14.506943 ]\n",
      " [14.506952 ]\n",
      " [14.506682 ]\n",
      " [14.507342 ]\n",
      " [14.507402 ]\n",
      " [14.507675 ]\n",
      " [14.508214 ]\n",
      " [14.507296 ]\n",
      " [14.5062275]\n",
      " [14.439    ]\n",
      " [14.507622 ]\n",
      " [14.507055 ]\n",
      " [14.514324 ]\n",
      " [14.4498625]\n",
      " [14.523104 ]\n",
      " [14.524891 ]]\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "RMSE: 0.00261864428211394\n",
      "MAE: 0.0016339600934450968\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "veri = pd.read_csv(r\"C:/Users/ASUS/Desktop/AI CLUB DATATHON/CSV/ail_frx.csv\")\n",
    "\n",
    "veri['open'] = veri['open'].str.replace('\"', '').astype(float)\n",
    "veri['high'] = veri['high'].str.replace('\"', '').astype(float)\n",
    "veri['low'] = veri['low'].str.replace('\"', '').astype(float)\n",
    "veri['close'] = veri['close'].str.replace('\"', '').astype(float)\n",
    "\n",
    "veri['open'].fillna(veri['open'].mean(), inplace=True)\n",
    "veri['high'].fillna(veri['high'].mean(), inplace=True)\n",
    "veri['low'].fillna(veri['low'].mean(), inplace=True)\n",
    "veri['close'].fillna(veri['close'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "veri = veri[['open', 'low', 'high', 'close']]\n",
    "\n",
    "X = veri[['close', 'open', 'high']].values\n",
    "y = veri['low'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_scaled, y_scaled, epochs=25, batch_size=32, verbose=1)\n",
    "\n",
    "gelecek_veri = X_scaled[-201:] \n",
    "tahminler = model.predict(gelecek_veri)\n",
    "\n",
    "\n",
    "tahminler = scaler.inverse_transform(tahminler)\n",
    "\n",
    "print(\"Gelecekteki Fiyat Tahminleri:\")\n",
    "print(tahminler)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01ad3506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0459\n",
      "Epoch 2/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0051e-06\n",
      "Epoch 3/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.4334e-07\n",
      "Epoch 4/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3760e-07\n",
      "Epoch 5/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.7649e-07\n",
      "Epoch 6/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5046e-07\n",
      "Epoch 7/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.8930e-07\n",
      "Epoch 8/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0901e-07\n",
      "Epoch 9/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.6161e-07\n",
      "Epoch 10/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2695e-06\n",
      "Epoch 11/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6158e-07\n",
      "Epoch 12/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6054e-07\n",
      "Epoch 13/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.0494e-06\n",
      "Epoch 14/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.0851e-07\n",
      "Epoch 15/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.0842e-07\n",
      "Epoch 16/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1421e-06\n",
      "Epoch 17/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.3798e-08\n",
      "Epoch 18/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0522e-06\n",
      "Epoch 19/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.8629e-07\n",
      "Epoch 20/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.2907e-06\n",
      "Epoch 21/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.7286e-06\n",
      "Epoch 22/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3526e-06\n",
      "Epoch 23/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3628e-05\n",
      "Epoch 24/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2759e-07\n",
      "Epoch 25/25\n",
      "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4876e-07\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Gelecekteki Fiyat Tahminleri:\n",
      "[[14.245327 ]\n",
      " [14.242742 ]\n",
      " [14.25019  ]\n",
      " [14.248877 ]\n",
      " [14.248027 ]\n",
      " [14.247018 ]\n",
      " [14.249766 ]\n",
      " [14.251161 ]\n",
      " [14.24859  ]\n",
      " [14.254551 ]\n",
      " [14.254421 ]\n",
      " [14.25936  ]\n",
      " [14.258871 ]\n",
      " [14.263444 ]\n",
      " [14.266914 ]\n",
      " [14.269058 ]\n",
      " [14.264736 ]\n",
      " [14.2668915]\n",
      " [14.268669 ]\n",
      " [ 9.937164 ]\n",
      " [14.254604 ]\n",
      " [14.282385 ]\n",
      " [14.287262 ]\n",
      " [14.286259 ]\n",
      " [14.2862215]\n",
      " [14.289896 ]\n",
      " [14.298087 ]\n",
      " [14.299222 ]\n",
      " [14.302453 ]\n",
      " [14.304588 ]\n",
      " [14.303806 ]\n",
      " [14.306985 ]\n",
      " [14.306754 ]\n",
      " [14.306972 ]\n",
      " [14.30582  ]\n",
      " [14.30519  ]\n",
      " [ 9.937077 ]\n",
      " [14.296471 ]\n",
      " [14.298399 ]\n",
      " [14.297739 ]\n",
      " [14.29599  ]\n",
      " [14.2936735]\n",
      " [14.3098545]\n",
      " [14.341825 ]\n",
      " [14.346879 ]\n",
      " [ 9.937212 ]\n",
      " [ 9.937205 ]\n",
      " [14.353946 ]\n",
      " [ 9.93728  ]\n",
      " [14.351195 ]\n",
      " [14.354583 ]\n",
      " [14.355508 ]\n",
      " [14.354511 ]\n",
      " [14.353707 ]\n",
      " [14.3562   ]\n",
      " [14.359046 ]\n",
      " [14.358816 ]\n",
      " [ 9.937238 ]\n",
      " [14.363243 ]\n",
      " [14.347876 ]\n",
      " [14.340597 ]\n",
      " [14.328246 ]\n",
      " [14.344483 ]\n",
      " [14.334461 ]\n",
      " [14.376925 ]\n",
      " [14.376521 ]\n",
      " [14.374313 ]\n",
      " [14.373195 ]\n",
      " [14.374669 ]\n",
      " [14.376617 ]\n",
      " [14.375126 ]\n",
      " [14.376198 ]\n",
      " [14.376116 ]\n",
      " [14.376818 ]\n",
      " [14.376522 ]\n",
      " [14.376175 ]\n",
      " [14.376336 ]\n",
      " [14.375905 ]\n",
      " [14.376289 ]\n",
      " [14.380788 ]\n",
      " [14.3778305]\n",
      " [14.379567 ]\n",
      " [14.383424 ]\n",
      " [14.378994 ]\n",
      " [14.379214 ]\n",
      " [ 9.937256 ]\n",
      " [14.385197 ]\n",
      " [14.390794 ]\n",
      " [14.395933 ]\n",
      " [ 9.937296 ]\n",
      " [14.393457 ]\n",
      " [14.394428 ]\n",
      " [14.394912 ]\n",
      " [ 9.937254 ]\n",
      " [14.395618 ]\n",
      " [14.395222 ]\n",
      " [14.394082 ]\n",
      " [ 9.937246 ]\n",
      " [14.39561  ]\n",
      " [14.396006 ]\n",
      " [14.395873 ]\n",
      " [14.39651  ]\n",
      " [14.399791 ]\n",
      " [14.398732 ]\n",
      " [14.404558 ]\n",
      " [14.403137 ]\n",
      " [14.402344 ]\n",
      " [14.394735 ]\n",
      " [14.398231 ]\n",
      " [14.400667 ]\n",
      " [14.3994665]\n",
      " [14.411894 ]\n",
      " [14.425948 ]\n",
      " [14.480757 ]\n",
      " [14.417612 ]\n",
      " [14.44209  ]\n",
      " [14.41267  ]\n",
      " [14.4156065]\n",
      " [14.41619  ]\n",
      " [14.4159155]\n",
      " [14.417256 ]\n",
      " [14.417037 ]\n",
      " [14.417921 ]\n",
      " [14.417455 ]\n",
      " [14.4186535]\n",
      " [14.420136 ]\n",
      " [14.418953 ]\n",
      " [14.418623 ]\n",
      " [14.418788 ]\n",
      " [14.415086 ]\n",
      " [14.410498 ]\n",
      " [14.4131155]\n",
      " [14.394306 ]\n",
      " [ 9.936829 ]\n",
      " [14.430076 ]\n",
      " [14.431245 ]\n",
      " [14.430923 ]\n",
      " [14.430892 ]\n",
      " [14.429256 ]\n",
      " [14.430428 ]\n",
      " [14.430557 ]\n",
      " [14.429913 ]\n",
      " [ 9.937212 ]\n",
      " [14.432052 ]\n",
      " [14.432179 ]\n",
      " [14.430978 ]\n",
      " [ 9.937283 ]\n",
      " [14.43122  ]\n",
      " [14.432046 ]\n",
      " [14.42941  ]\n",
      " [14.4299   ]\n",
      " [ 9.937174 ]\n",
      " [14.431658 ]\n",
      " [14.433568 ]\n",
      " [14.467821 ]\n",
      " [14.465318 ]\n",
      " [14.466438 ]\n",
      " [ 9.937229 ]\n",
      " [14.682763 ]\n",
      " [ 9.948185 ]\n",
      " [14.46753  ]\n",
      " [14.474325 ]\n",
      " [14.464332 ]\n",
      " [14.468856 ]\n",
      " [14.469473 ]\n",
      " [14.470972 ]\n",
      " [14.470945 ]\n",
      " [14.4731045]\n",
      " [14.472972 ]\n",
      " [14.473156 ]\n",
      " [14.471646 ]\n",
      " [14.471724 ]\n",
      " [14.45963  ]\n",
      " [14.450654 ]\n",
      " [14.453568 ]\n",
      " [14.411774 ]\n",
      " [14.431269 ]\n",
      " [14.485884 ]\n",
      " [14.514363 ]\n",
      " [14.512608 ]\n",
      " [14.5113125]\n",
      " [14.511154 ]\n",
      " [ 9.937832 ]\n",
      " [14.508514 ]\n",
      " [14.510259 ]\n",
      " [14.510249 ]\n",
      " [14.51153  ]\n",
      " [14.510822 ]\n",
      " [14.510973 ]\n",
      " [14.510752 ]\n",
      " [14.5117445]\n",
      " [14.510882 ]\n",
      " [14.510395 ]\n",
      " [14.510301 ]\n",
      " [ 9.937806 ]\n",
      " [14.510342 ]\n",
      " [14.510727 ]\n",
      " [14.500871 ]\n",
      " [ 9.936631 ]\n",
      " [14.527299 ]\n",
      " [14.529329 ]]\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "RMSE: 0.0001340586521505379\n",
      "MAE: 0.00010264055692906284\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "veri = pd.read_csv(r\"C:/Users/ASUS/Desktop/AI CLUB DATATHON/CSV/ail_frx.csv\")\n",
    "\n",
    "veri['open'] = veri['open'].str.replace('\"', '').astype(float)\n",
    "veri['high'] = veri['high'].str.replace('\"', '').astype(float)\n",
    "veri['low'] = veri['low'].str.replace('\"', '').astype(float)\n",
    "veri['close'] = veri['close'].str.replace('\"', '').astype(float)\n",
    "\n",
    "\n",
    "veri['open'].fillna(veri['open'].mean(), inplace=True)\n",
    "veri['high'].fillna(veri['high'].mean(), inplace=True)\n",
    "veri['low'].fillna(veri['low'].mean(), inplace=True)\n",
    "veri['close'].fillna(veri['close'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "veri = veri[['open', 'low', 'high', 'close']]\n",
    "\n",
    "\n",
    "X = veri[['close', 'open', 'high']].values\n",
    "y = veri['close'].values\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "\n",
    "model.fit(X_scaled, y_scaled, epochs=25, batch_size=32, verbose=1)\n",
    "\n",
    "\n",
    "gelecek_veri = X_scaled[-201:]  \n",
    "tahminler = model.predict(gelecek_veri)\n",
    "\n",
    "\n",
    "tahminler = scaler.inverse_transform(tahminler)\n",
    "\n",
    "print(\"Gelecekteki Fiyat Tahminleri:\")\n",
    "print(tahminler)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e08ca6",
   "metadata": {},
   "source": [
    "**crp_ail kısmı bu kodlar aracılığı ile yapıldı.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c554f07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0134\n",
      "Epoch 2/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0032\n",
      "Epoch 3/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0035\n",
      "Epoch 4/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0031\n",
      "Epoch 5/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0032\n",
      "Epoch 6/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0031\n",
      "Epoch 7/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0034\n",
      "Epoch 8/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0037\n",
      "Epoch 9/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0037\n",
      "Epoch 10/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0034\n",
      "Epoch 11/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0034\n",
      "Epoch 12/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0033\n",
      "Epoch 13/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0033\n",
      "Epoch 14/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0034\n",
      "Epoch 15/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0031\n",
      "Epoch 16/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0031\n",
      "Epoch 17/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0032\n",
      "Epoch 18/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0031\n",
      "Epoch 19/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0038\n",
      "Epoch 20/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0034\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Gelecekteki Fiyat Tahminleri:\n",
      "[[302.71118]\n",
      " [302.5584 ]\n",
      " [302.4678 ]\n",
      " [302.44077]\n",
      " [302.36328]\n",
      " [302.29083]\n",
      " [302.13602]\n",
      " [302.15274]\n",
      " [302.2982 ]\n",
      " [302.2537 ]\n",
      " [302.3346 ]\n",
      " [302.31302]\n",
      " [303.16376]\n",
      " [303.55057]\n",
      " [304.47092]\n",
      " [304.0437 ]\n",
      " [304.81683]\n",
      " [304.90115]\n",
      " [305.18546]\n",
      " [302.97073]\n",
      " [305.13684]\n",
      " [305.2045 ]\n",
      " [304.964  ]\n",
      " [305.27948]\n",
      " [305.54535]\n",
      " [306.32504]\n",
      " [306.28555]\n",
      " [304.0476 ]\n",
      " [306.35812]\n",
      " [306.2134 ]\n",
      " [304.45078]\n",
      " [306.1448 ]\n",
      " [306.26617]\n",
      " [306.89972]\n",
      " [305.97653]\n",
      " [305.97095]\n",
      " [306.34097]\n",
      " [306.4167 ]\n",
      " [306.54343]\n",
      " [306.61942]\n",
      " [306.6148 ]\n",
      " [306.69037]\n",
      " [306.83087]\n",
      " [306.9505 ]\n",
      " [306.69928]\n",
      " [306.29266]\n",
      " [306.2607 ]\n",
      " [306.3829 ]\n",
      " [306.65143]\n",
      " [306.79385]\n",
      " [307.41068]\n",
      " [307.79355]\n",
      " [307.8712 ]\n",
      " [307.78925]\n",
      " [307.74637]\n",
      " [308.3831 ]\n",
      " [308.02277]\n",
      " [308.4479 ]\n",
      " [308.96317]\n",
      " [310.4981 ]\n",
      " [309.34644]\n",
      " [308.50623]\n",
      " [308.97342]\n",
      " [309.15387]\n",
      " [309.2964 ]\n",
      " [307.22144]\n",
      " [309.99265]\n",
      " [309.77966]\n",
      " [309.20184]\n",
      " [309.2088 ]\n",
      " [309.44058]\n",
      " [309.29105]\n",
      " [309.20477]\n",
      " [309.43918]\n",
      " [309.37573]\n",
      " [309.23865]\n",
      " [309.22748]\n",
      " [309.116  ]\n",
      " [309.193  ]\n",
      " [309.42392]\n",
      " [309.35312]\n",
      " [309.77744]\n",
      " [309.61346]\n",
      " [309.5499 ]\n",
      " [309.26675]\n",
      " [309.6984 ]\n",
      " [309.6406 ]\n",
      " [309.71375]\n",
      " [309.75308]\n",
      " [309.70328]\n",
      " [309.5902 ]\n",
      " [309.48566]\n",
      " [309.10495]\n",
      " [309.14084]\n",
      " [308.9551 ]\n",
      " [308.77463]\n",
      " [308.8638 ]\n",
      " [309.01093]\n",
      " [309.07498]\n",
      " [308.91916]\n",
      " [308.54132]\n",
      " [308.381  ]\n",
      " [307.1792 ]\n",
      " [306.9235 ]\n",
      " [307.27612]\n",
      " [306.8153 ]\n",
      " [306.03375]\n",
      " [305.95984]\n",
      " [306.40826]\n",
      " [306.49313]\n",
      " [306.5877 ]\n",
      " [306.3989 ]\n",
      " [306.54538]\n",
      " [306.52167]\n",
      " [306.67468]\n",
      " [306.61118]\n",
      " [306.5244 ]\n",
      " [306.67554]\n",
      " [306.71628]\n",
      " [306.68274]\n",
      " [304.6995 ]\n",
      " [306.57925]\n",
      " [306.86758]\n",
      " [307.1107 ]\n",
      " [307.29825]\n",
      " [307.63928]\n",
      " [308.25906]\n",
      " [308.36102]\n",
      " [308.9467 ]\n",
      " [308.7693 ]\n",
      " [308.5441 ]\n",
      " [308.62326]\n",
      " [308.66455]\n",
      " [308.7685 ]\n",
      " [308.75977]\n",
      " [308.7375 ]\n",
      " [308.58823]\n",
      " [308.28275]\n",
      " [308.09772]\n",
      " [307.89673]\n",
      " [307.81332]\n",
      " [307.93347]\n",
      " [308.05444]\n",
      " [307.99612]\n",
      " [307.92725]\n",
      " [307.46918]\n",
      " [307.13922]\n",
      " [306.85098]\n",
      " [306.43335]\n",
      " [306.53635]\n",
      " [306.95746]\n",
      " [306.89584]\n",
      " [307.00403]\n",
      " [307.09268]\n",
      " [306.97614]\n",
      " [306.9731 ]\n",
      " [306.9168 ]\n",
      " [306.97253]\n",
      " [307.2831 ]\n",
      " [307.0247 ]\n",
      " [305.1517 ]\n",
      " [307.337  ]\n",
      " [307.59897]\n",
      " [307.8759 ]\n",
      " [307.83136]\n",
      " [307.71957]\n",
      " [308.08875]\n",
      " [308.14465]\n",
      " [307.7586 ]\n",
      " [307.09882]\n",
      " [306.7474 ]\n",
      " [306.908  ]\n",
      " [306.83752]\n",
      " [306.46112]\n",
      " [306.5236 ]\n",
      " [306.45364]\n",
      " [306.31418]\n",
      " [305.9411 ]\n",
      " [306.12387]\n",
      " [306.23428]\n",
      " [303.9462 ]\n",
      " [305.00873]\n",
      " [305.02844]\n",
      " [305.03036]\n",
      " [305.12247]\n",
      " [305.38367]\n",
      " [305.713  ]\n",
      " [306.0129 ]\n",
      " [306.27936]\n",
      " [306.70395]\n",
      " [306.86816]\n",
      " [304.54895]\n",
      " [306.6244 ]\n",
      " [306.44397]\n",
      " [306.6805 ]\n",
      " [306.70712]\n",
      " [306.74442]\n",
      " [306.70386]\n",
      " [306.75064]\n",
      " [306.89584]\n",
      " [306.89642]]\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "RMSE: 0.06217185445865172\n",
      "MAE: 0.027066357016339693\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "veri = pd.read_csv(r\"C:/Users/ASUS/Desktop/AI CLUB DATATHON/CSV/crp_ail.csv\")\n",
    "\n",
    "veri['open'] = veri['open'].str.replace('\"', '').astype(float)\n",
    "veri['high'] = veri['high'].str.replace('\"', '').astype(float)\n",
    "veri['low'] = veri['low'].str.replace('\"', '').astype(float)\n",
    "veri['close'] = veri['close'].str.replace('\"', '').astype(float)\n",
    "\n",
    "\n",
    "veri['open'].fillna(veri['open'].mean(), inplace=True)\n",
    "veri['high'].fillna(veri['high'].mean(), inplace=True)\n",
    "veri['low'].fillna(veri['low'].mean(), inplace=True)\n",
    "veri['close'].fillna(veri['close'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "veri = veri[['open', 'low', 'high', 'close']]\n",
    "\n",
    "X = veri[['close', 'low', 'high']].values\n",
    "y = veri['open'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_scaled, y_scaled, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "gelecek_veri = X_scaled[-201:] \n",
    "tahminler = model.predict(gelecek_veri)\n",
    "\n",
    "tahminler = scaler.inverse_transform(tahminler)\n",
    "\n",
    "print(\"Gelecekteki Fiyat Tahminleri:\")\n",
    "print(tahminler)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b81d5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0247\n",
      "Epoch 2/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3462e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.8164e-06\n",
      "Epoch 4/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.0121e-06\n",
      "Epoch 5/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.7155e-06\n",
      "Epoch 6/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.9056e-06\n",
      "Epoch 7/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.4036e-06\n",
      "Epoch 8/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.8157e-06\n",
      "Epoch 9/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6242e-06\n",
      "Epoch 10/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.9687e-06\n",
      "Epoch 11/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.8634e-06\n",
      "Epoch 12/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.4455e-07\n",
      "Epoch 13/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.2270e-07\n",
      "Epoch 14/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.2134e-07\n",
      "Epoch 15/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.2863e-07\n",
      "Epoch 16/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6657e-07\n",
      "Epoch 17/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.4710e-07\n",
      "Epoch 18/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.0628e-07\n",
      "Epoch 19/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.6238e-07\n",
      "Epoch 20/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.8817e-07\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "Gelecekteki Fiyat Tahminleri:\n",
      "[[304.3568 ]\n",
      " [304.2312 ]\n",
      " [303.95346]\n",
      " [304.00488]\n",
      " [303.95926]\n",
      " [303.9359 ]\n",
      " [303.76617]\n",
      " [303.7061 ]\n",
      " [303.93616]\n",
      " [303.8753 ]\n",
      " [304.1085 ]\n",
      " [304.0291 ]\n",
      " [304.94028]\n",
      " [305.5903 ]\n",
      " [306.9397 ]\n",
      " [306.43857]\n",
      " [307.04343]\n",
      " [307.06985]\n",
      " [307.3823 ]\n",
      " [306.73523]\n",
      " [306.8234 ]\n",
      " [307.03   ]\n",
      " [306.926  ]\n",
      " [307.36792]\n",
      " [307.43005]\n",
      " [308.7068 ]\n",
      " [308.32184]\n",
      " [307.9275 ]\n",
      " [308.4707 ]\n",
      " [308.1869 ]\n",
      " [308.28555]\n",
      " [308.17746]\n",
      " [308.32953]\n",
      " [309.176  ]\n",
      " [308.44156]\n",
      " [308.28006]\n",
      " [308.33362]\n",
      " [308.3984 ]\n",
      " [308.621  ]\n",
      " [308.57547]\n",
      " [308.52707]\n",
      " [308.51328]\n",
      " [308.8729 ]\n",
      " [309.011  ]\n",
      " [308.75626]\n",
      " [308.27936]\n",
      " [308.31964]\n",
      " [308.2786 ]\n",
      " [308.79462]\n",
      " [308.775  ]\n",
      " [309.73816]\n",
      " [310.07355]\n",
      " [310.0033 ]\n",
      " [309.88306]\n",
      " [309.8304 ]\n",
      " [312.07095]\n",
      " [310.58536]\n",
      " [310.95822]\n",
      " [311.45844]\n",
      " [313.49496]\n",
      " [312.04874]\n",
      " [311.03067]\n",
      " [311.10013]\n",
      " [311.19986]\n",
      " [311.56427]\n",
      " [311.43585]\n",
      " [312.6367 ]\n",
      " [312.29776]\n",
      " [311.41464]\n",
      " [311.4053 ]\n",
      " [311.58856]\n",
      " [311.58893]\n",
      " [311.36682]\n",
      " [311.73373]\n",
      " [311.77542]\n",
      " [311.57468]\n",
      " [311.46387]\n",
      " [311.2902 ]\n",
      " [311.45157]\n",
      " [312.07242]\n",
      " [311.81332]\n",
      " [312.16263]\n",
      " [312.0145 ]\n",
      " [311.93353]\n",
      " [311.49432]\n",
      " [311.94315]\n",
      " [311.80804]\n",
      " [311.7964 ]\n",
      " [312.0403 ]\n",
      " [312.004  ]\n",
      " [311.8173 ]\n",
      " [311.76154]\n",
      " [311.22906]\n",
      " [311.2589 ]\n",
      " [311.23596]\n",
      " [310.9508 ]\n",
      " [310.99133]\n",
      " [311.1368 ]\n",
      " [311.22656]\n",
      " [311.02505]\n",
      " [310.8622 ]\n",
      " [311.59   ]\n",
      " [309.86197]\n",
      " [309.1935 ]\n",
      " [309.42297]\n",
      " [309.31348]\n",
      " [308.00922]\n",
      " [308.11694]\n",
      " [308.35953]\n",
      " [308.27847]\n",
      " [308.70535]\n",
      " [308.38672]\n",
      " [308.57278]\n",
      " [308.4464 ]\n",
      " [308.72784]\n",
      " [308.60416]\n",
      " [308.46307]\n",
      " [308.59274]\n",
      " [308.70453]\n",
      " [308.6192 ]\n",
      " [308.57367]\n",
      " [308.53537]\n",
      " [309.02322]\n",
      " [309.1038 ]\n",
      " [309.40442]\n",
      " [310.1368 ]\n",
      " [310.57056]\n",
      " [310.59778]\n",
      " [311.33078]\n",
      " [310.86783]\n",
      " [310.72577]\n",
      " [310.60236]\n",
      " [310.7178 ]\n",
      " [310.82755]\n",
      " [310.87854]\n",
      " [310.9301 ]\n",
      " [310.7136 ]\n",
      " [310.2956 ]\n",
      " [310.21292]\n",
      " [309.96146]\n",
      " [309.79153]\n",
      " [309.9463 ]\n",
      " [310.21274]\n",
      " [310.05893]\n",
      " [310.05637]\n",
      " [309.9374 ]\n",
      " [309.50888]\n",
      " [309.04187]\n",
      " [308.8031 ]\n",
      " [308.54346]\n",
      " [309.1988 ]\n",
      " [308.8449 ]\n",
      " [308.99786]\n",
      " [309.0576 ]\n",
      " [308.80096]\n",
      " [308.8546 ]\n",
      " [308.81122]\n",
      " [308.9498 ]\n",
      " [309.35278]\n",
      " [308.93237]\n",
      " [309.0574 ]\n",
      " [309.28033]\n",
      " [309.81824]\n",
      " [309.94968]\n",
      " [309.9232 ]\n",
      " [309.84882]\n",
      " [310.3161 ]\n",
      " [310.30566]\n",
      " [309.83087]\n",
      " [309.53638]\n",
      " [308.9746 ]\n",
      " [309.10458]\n",
      " [308.99442]\n",
      " [308.58344]\n",
      " [308.54028]\n",
      " [308.42593]\n",
      " [308.26086]\n",
      " [307.94797]\n",
      " [308.065  ]\n",
      " [308.1504 ]\n",
      " [307.8904 ]\n",
      " [306.8049 ]\n",
      " [306.8346 ]\n",
      " [306.83466]\n",
      " [306.947  ]\n",
      " [307.38278]\n",
      " [307.69827]\n",
      " [307.91608]\n",
      " [308.28583]\n",
      " [308.6862 ]\n",
      " [309.00934]\n",
      " [308.50143]\n",
      " [308.6846 ]\n",
      " [308.60498]\n",
      " [308.67014]\n",
      " [308.6021 ]\n",
      " [308.64764]\n",
      " [308.56342]\n",
      " [308.55823]\n",
      " [308.86697]\n",
      " [308.81738]]\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "RMSE: 0.0007563394079618845\n",
      "MAE: 0.0007179458762785808\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "veri = pd.read_csv(r\"C:/Users/ASUS/Desktop/AI CLUB DATATHON/CSV/crp_ail.csv\")\n",
    "\n",
    "veri['open'] = veri['open'].str.replace('\"', '').astype(float)\n",
    "veri['high'] = veri['high'].str.replace('\"', '').astype(float)\n",
    "veri['low'] = veri['low'].str.replace('\"', '').astype(float)\n",
    "veri['close'] = veri['close'].str.replace('\"', '').astype(float)\n",
    "\n",
    "\n",
    "veri['open'].fillna(veri['open'].mean(), inplace=True)\n",
    "veri['high'].fillna(veri['high'].mean(), inplace=True)\n",
    "veri['low'].fillna(veri['low'].mean(), inplace=True)\n",
    "veri['close'].fillna(veri['close'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "veri = veri[['open', 'low', 'high', 'close']]\n",
    "\n",
    "X = veri[['close', 'low', 'high']].values\n",
    "y = veri['high'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_scaled, y_scaled, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "gelecek_veri = X_scaled[-201:]  \n",
    "tahminler = model.predict(gelecek_veri)\n",
    "\n",
    "\n",
    "tahminler = scaler.inverse_transform(tahminler)\n",
    "\n",
    "print(\"Gelecekteki Fiyat Tahminleri:\")\n",
    "print(tahminler)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bdcae8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0365\n",
      "Epoch 2/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6020e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5432e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.7915e-06\n",
      "Epoch 5/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.0072e-06\n",
      "Epoch 6/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.8594e-06\n",
      "Epoch 7/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3884e-06\n",
      "Epoch 8/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.7009e-06\n",
      "Epoch 9/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.6841e-07\n",
      "Epoch 10/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.1009e-07\n",
      "Epoch 11/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6750e-07\n",
      "Epoch 12/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.7685e-07\n",
      "Epoch 13/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.6991e-07\n",
      "Epoch 14/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.7751e-07\n",
      "Epoch 15/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.9043e-07\n",
      "Epoch 16/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0052e-06\n",
      "Epoch 17/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5541e-07\n",
      "Epoch 18/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.0056e-07\n",
      "Epoch 19/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.1724e-07\n",
      "Epoch 20/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.8163e-06\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Gelecekteki Fiyat Tahminleri:\n",
      "[[304.11304]\n",
      " [303.90225]\n",
      " [304.025  ]\n",
      " [303.89487]\n",
      " [303.76694]\n",
      " [303.625  ]\n",
      " [303.45346]\n",
      " [303.56934]\n",
      " [303.64346]\n",
      " [303.60175]\n",
      " [303.51993]\n",
      " [303.5633 ]\n",
      " [304.476  ]\n",
      " [304.5941 ]\n",
      " [305.14713]\n",
      " [304.74075]\n",
      " [305.84985]\n",
      " [306.03748]\n",
      " [306.33472]\n",
      " [306.2368 ]\n",
      " [306.9035 ]\n",
      " [306.81   ]\n",
      " [306.36255]\n",
      " [306.56705]\n",
      " [307.13483]\n",
      " [307.43054]\n",
      " [307.82645]\n",
      " [307.4503 ]\n",
      " [307.81616]\n",
      " [307.804  ]\n",
      " [308.00848]\n",
      " [307.6551 ]\n",
      " [307.75253]\n",
      " [308.24673]\n",
      " [306.92526]\n",
      " [307.10263]\n",
      " [307.93127]\n",
      " [308.04108]\n",
      " [308.06244]\n",
      " [308.3104 ]\n",
      " [308.35248]\n",
      " [308.55524]\n",
      " [308.44434]\n",
      " [308.57025]\n",
      " [308.28482]\n",
      " [307.89273]\n",
      " [307.75534]\n",
      " [308.10394]\n",
      " [308.1062 ]\n",
      " [308.4777 ]\n",
      " [308.77548]\n",
      " [309.2785 ]\n",
      " [309.5574 ]\n",
      " [309.50687]\n",
      " [309.4768 ]\n",
      " [308.22684]\n",
      " [309.1971 ]\n",
      " [309.74707]\n",
      " [310.36966]\n",
      " [311.5539 ]\n",
      " [310.59753]\n",
      " [309.80298]\n",
      " [310.85306]\n",
      " [311.1982 ]\n",
      " [311.0679 ]\n",
      " [310.73196]\n",
      " [311.3882 ]\n",
      " [311.32428]\n",
      " [311.0214 ]\n",
      " [311.04105]\n",
      " [311.39685]\n",
      " [311.02124]\n",
      " [311.08612]\n",
      " [311.1852 ]\n",
      " [310.99997]\n",
      " [310.90045]\n",
      " [311.02173]\n",
      " [310.95938]\n",
      " [310.95502]\n",
      " [310.72324]\n",
      " [310.8769 ]\n",
      " [311.47116]\n",
      " [311.26965]\n",
      " [311.21948]\n",
      " [311.0672 ]\n",
      " [311.5627 ]\n",
      " [311.61816]\n",
      " [311.83496]\n",
      " [311.5762 ]\n",
      " [311.50366]\n",
      " [311.46655]\n",
      " [311.27777]\n",
      " [311.0143 ]\n",
      " [311.06976]\n",
      " [310.64566]\n",
      " [310.56128]\n",
      " [310.72177]\n",
      " [310.90012]\n",
      " [310.94623]\n",
      " [310.8171 ]\n",
      " [310.11542]\n",
      " [308.81384]\n",
      " [308.0774 ]\n",
      " [308.26938]\n",
      " [308.83408]\n",
      " [307.8906 ]\n",
      " [307.6029 ]\n",
      " [307.273  ]\n",
      " [308.0634 ]\n",
      " [308.3706 ]\n",
      " [308.07162]\n",
      " [308.0126 ]\n",
      " [308.1302 ]\n",
      " [308.2314 ]\n",
      " [308.24902]\n",
      " [308.2524 ]\n",
      " [308.21448]\n",
      " [308.41965]\n",
      " [308.38364]\n",
      " [308.4057 ]\n",
      " [308.27124]\n",
      " [308.26187]\n",
      " [308.34186]\n",
      " [308.8332 ]\n",
      " [308.92056]\n",
      " [308.83658]\n",
      " [309.78842]\n",
      " [309.99564]\n",
      " [310.51456]\n",
      " [310.6529 ]\n",
      " [310.28882]\n",
      " [310.64233]\n",
      " [310.58368]\n",
      " [310.69937]\n",
      " [310.61032]\n",
      " [310.49878]\n",
      " [310.41074]\n",
      " [310.18637]\n",
      " [309.84976]\n",
      " [309.67056]\n",
      " [309.67783]\n",
      " [309.77536]\n",
      " [309.73904]\n",
      " [309.78467]\n",
      " [309.62872]\n",
      " [308.68207]\n",
      " [308.40796]\n",
      " [308.2839 ]\n",
      " [307.58026]\n",
      " [308.147  ]\n",
      " [308.35754]\n",
      " [308.63992]\n",
      " [308.70618]\n",
      " [308.85422]\n",
      " [308.8878 ]\n",
      " [308.81586]\n",
      " [308.73184]\n",
      " [308.69168]\n",
      " [308.95145]\n",
      " [308.84192]\n",
      " [308.75793]\n",
      " [309.16498]\n",
      " [309.1191 ]\n",
      " [309.63358]\n",
      " [309.56543]\n",
      " [309.3761 ]\n",
      " [309.68723]\n",
      " [309.8476 ]\n",
      " [309.5023 ]\n",
      " [308.2777 ]\n",
      " [308.1218 ]\n",
      " [308.354  ]\n",
      " [308.32346]\n",
      " [307.9219 ]\n",
      " [308.1172 ]\n",
      " [308.09802]\n",
      " [307.96634]\n",
      " [307.44632]\n",
      " [307.74072]\n",
      " [307.90448]\n",
      " [307.254  ]\n",
      " [306.61978]\n",
      " [306.62848]\n",
      " [306.63416]\n",
      " [306.7158 ]\n",
      " [306.79684]\n",
      " [307.2048 ]\n",
      " [307.6617 ]\n",
      " [307.84015]\n",
      " [308.36923]\n",
      " [308.37888]\n",
      " [307.999  ]\n",
      " [308.18784]\n",
      " [307.8396 ]\n",
      " [308.33655]\n",
      " [308.48785]\n",
      " [308.51935]\n",
      " [308.52438]\n",
      " [308.6449 ]\n",
      " [308.61313]\n",
      " [308.6723 ]]\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "RMSE: 0.00037814870053846196\n",
      "MAE: 0.0003494904348502573\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "veri = pd.read_csv(r\"C:/Users/ASUS/Desktop/AI CLUB DATATHON/CSV/crp_ail.csv\")\n",
    "\n",
    "veri['open'] = veri['open'].str.replace('\"', '').astype(float)\n",
    "veri['high'] = veri['high'].str.replace('\"', '').astype(float)\n",
    "veri['low'] = veri['low'].str.replace('\"', '').astype(float)\n",
    "veri['close'] = veri['close'].str.replace('\"', '').astype(float)\n",
    "\n",
    "veri['open'].fillna(veri['open'].mean(), inplace=True)\n",
    "veri['high'].fillna(veri['high'].mean(), inplace=True)\n",
    "veri['low'].fillna(veri['low'].mean(), inplace=True)\n",
    "veri['close'].fillna(veri['close'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "veri = veri[['open', 'low', 'high', 'close']]\n",
    "\n",
    "X = veri[['close', 'low', 'high']].values\n",
    "y = veri['low'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_scaled, y_scaled, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "gelecek_veri = X_scaled[-201:]  \n",
    "tahminler = model.predict(gelecek_veri)\n",
    "\n",
    "tahminler = scaler.inverse_transform(tahminler)\n",
    "\n",
    "print(\"Gelecekteki Fiyat Tahminleri:\")\n",
    "print(tahminler)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d297dcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0369\n",
      "Epoch 2/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.9132e-06\n",
      "Epoch 3/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5659e-06\n",
      "Epoch 4/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0600e-06\n",
      "Epoch 5/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.4161e-07\n",
      "Epoch 6/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.9794e-07\n",
      "Epoch 7/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.3842e-07\n",
      "Epoch 8/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.8209e-07\n",
      "Epoch 9/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.5849e-07\n",
      "Epoch 10/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.1465e-07\n",
      "Epoch 11/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.4596e-07\n",
      "Epoch 12/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6183e-07\n",
      "Epoch 13/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.7326e-06\n",
      "Epoch 14/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.6759e-07\n",
      "Epoch 15/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.7310e-07\n",
      "Epoch 16/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.7283e-06\n",
      "Epoch 17/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.9965e-07\n",
      "Epoch 18/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.5928e-06\n",
      "Epoch 19/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.8650e-08\n",
      "Epoch 20/20\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.8152e-08\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Gelecekteki Fiyat Tahminleri:\n",
      "[[304.3181 ]\n",
      " [304.09055]\n",
      " [304.08466]\n",
      " [304.09818]\n",
      " [303.93784]\n",
      " [303.7332 ]\n",
      " [303.8583 ]\n",
      " [303.83618]\n",
      " [303.7076 ]\n",
      " [303.9915 ]\n",
      " [303.67664]\n",
      " [303.75266]\n",
      " [304.98486]\n",
      " [305.72015]\n",
      " [306.5244 ]\n",
      " [305.97   ]\n",
      " [306.9756 ]\n",
      " [306.48846]\n",
      " [306.83017]\n",
      " [270.4711 ]\n",
      " [306.97546]\n",
      " [307.0773 ]\n",
      " [306.70297]\n",
      " [307.33926]\n",
      " [307.48883]\n",
      " [308.42188]\n",
      " [307.8853 ]\n",
      " [270.49118]\n",
      " [307.99478]\n",
      " [308.33643]\n",
      " [270.5042 ]\n",
      " [308.10315]\n",
      " [308.46143]\n",
      " [308.62732]\n",
      " [307.83505]\n",
      " [308.1345 ]\n",
      " [308.42087]\n",
      " [308.25003]\n",
      " [308.67337]\n",
      " [308.46777]\n",
      " [308.68784]\n",
      " [308.64746]\n",
      " [308.94574]\n",
      " [308.7848 ]\n",
      " [308.3588 ]\n",
      " [308.00705]\n",
      " [308.33017]\n",
      " [308.34906]\n",
      " [308.84158]\n",
      " [308.8605 ]\n",
      " [309.40045]\n",
      " [309.96286]\n",
      " [309.88306]\n",
      " [309.8574 ]\n",
      " [309.53918]\n",
      " [310.28006]\n",
      " [310.21667]\n",
      " [311.11166]\n",
      " [311.6343 ]\n",
      " [312.27316]\n",
      " [310.73654]\n",
      " [311.0019 ]\n",
      " [311.15805]\n",
      " [311.33646]\n",
      " [311.1309 ]\n",
      " [270.59695]\n",
      " [312.48834]\n",
      " [311.5642 ]\n",
      " [311.16238]\n",
      " [311.46216]\n",
      " [311.63043]\n",
      " [311.23618]\n",
      " [311.4582 ]\n",
      " [311.85873]\n",
      " [311.1429 ]\n",
      " [311.59235]\n",
      " [311.2065 ]\n",
      " [311.3807 ]\n",
      " [311.13202]\n",
      " [311.78015]\n",
      " [311.80856]\n",
      " [312.13736]\n",
      " [311.64163]\n",
      " [311.48035]\n",
      " [311.63614]\n",
      " [311.73022]\n",
      " [311.92493]\n",
      " [311.90607]\n",
      " [311.72437]\n",
      " [311.58426]\n",
      " [311.92844]\n",
      " [311.40256]\n",
      " [311.16995]\n",
      " [311.27573]\n",
      " [310.97693]\n",
      " [310.8659 ]\n",
      " [311.10202]\n",
      " [311.22098]\n",
      " [311.1011 ]\n",
      " [311.03818]\n",
      " [310.38882]\n",
      " [310.10413]\n",
      " [308.71478]\n",
      " [309.08826]\n",
      " [309.50565]\n",
      " [307.94894]\n",
      " [307.74228]\n",
      " [308.2841 ]\n",
      " [308.42572]\n",
      " [308.43594]\n",
      " [308.48773]\n",
      " [308.2328 ]\n",
      " [308.55966]\n",
      " [308.46915]\n",
      " [308.72684]\n",
      " [308.54593]\n",
      " [308.57318]\n",
      " [308.68372]\n",
      " [308.58627]\n",
      " [308.64365]\n",
      " [270.50803]\n",
      " [308.47192]\n",
      " [309.15176]\n",
      " [309.23245]\n",
      " [309.18222]\n",
      " [309.52396]\n",
      " [310.34943]\n",
      " [310.62143]\n",
      " [310.74637]\n",
      " [310.79663]\n",
      " [310.46277]\n",
      " [310.75156]\n",
      " [310.77097]\n",
      " [310.89398]\n",
      " [311.00046]\n",
      " [310.77863]\n",
      " [310.4962 ]\n",
      " [310.35623]\n",
      " [309.92392]\n",
      " [309.9151 ]\n",
      " [309.89694]\n",
      " [310.04117]\n",
      " [310.1017 ]\n",
      " [310.18365]\n",
      " [309.87756]\n",
      " [309.05   ]\n",
      " [309.0154 ]\n",
      " [308.95743]\n",
      " [308.26825]\n",
      " [308.47278]\n",
      " [308.69342]\n",
      " [308.86826]\n",
      " [309.15646]\n",
      " [308.9687 ]\n",
      " [308.95377]\n",
      " [308.8919 ]\n",
      " [308.90305]\n",
      " [309.05948]\n",
      " [309.04977]\n",
      " [309.01315]\n",
      " [270.51752]\n",
      " [309.30923]\n",
      " [309.97556]\n",
      " [309.93774]\n",
      " [309.67636]\n",
      " [309.961  ]\n",
      " [310.3595 ]\n",
      " [309.99216]\n",
      " [309.6663 ]\n",
      " [308.9334 ]\n",
      " [308.70566]\n",
      " [308.67224]\n",
      " [308.49683]\n",
      " [308.20337]\n",
      " [308.56253]\n",
      " [308.22025]\n",
      " [308.0702 ]\n",
      " [307.95   ]\n",
      " [308.18375]\n",
      " [308.18716]\n",
      " [270.4846 ]\n",
      " [306.76868]\n",
      " [306.85995]\n",
      " [306.8229 ]\n",
      " [306.96286]\n",
      " [307.5458 ]\n",
      " [307.73682]\n",
      " [307.92972]\n",
      " [308.41483]\n",
      " [308.8231 ]\n",
      " [308.5313 ]\n",
      " [270.49933]\n",
      " [308.45694]\n",
      " [308.66806]\n",
      " [308.6646 ]\n",
      " [308.5927 ]\n",
      " [308.69977]\n",
      " [308.7057 ]\n",
      " [308.71057]\n",
      " [308.85242]\n",
      " [308.9755 ]]\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "RMSE: 0.0006083523823731347\n",
      "MAE: 0.0005342891759797481\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "veri = pd.read_csv(r\"C:/Users/ASUS/Desktop/AI CLUB DATATHON/CSV/crp_ail.csv\")\n",
    "\n",
    "veri['open'] = veri['open'].str.replace('\"', '').astype(float)\n",
    "veri['high'] = veri['high'].str.replace('\"', '').astype(float)\n",
    "veri['low'] = veri['low'].str.replace('\"', '').astype(float)\n",
    "veri['close'] = veri['close'].str.replace('\"', '').astype(float)\n",
    "\n",
    "veri['open'].fillna(veri['open'].mean(), inplace=True)\n",
    "veri['high'].fillna(veri['high'].mean(), inplace=True)\n",
    "veri['low'].fillna(veri['low'].mean(), inplace=True)\n",
    "veri['close'].fillna(veri['close'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "veri = veri[['open', 'low', 'high', 'close']]\n",
    "\n",
    "X = veri[['close', 'low', 'high']].values\n",
    "y = veri['close'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_scaled, y_scaled, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "\n",
    "gelecek_veri = X_scaled[-201:]  \n",
    "tahminler = model.predict(gelecek_veri)\n",
    "\n",
    "\n",
    "tahminler = scaler.inverse_transform(tahminler)\n",
    "\n",
    "print(\"Gelecekteki Fiyat Tahminleri:\")\n",
    "print(tahminler)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b751fb9",
   "metadata": {},
   "source": [
    "**bk_frx kısmı bu kodlar aracılığı ile yapıldı.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2b488c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor ile Gelecekteki Fiyat Tahminleri:\n",
      "[20140.377913  20190.1059696 20200.236113  20240.2605609 20321.4754565\n",
      " 20327.9324696 20231.0618348 20380.6681609 20455.706713  20476.9304478\n",
      " 20463.8101826 20519.492213  20524.4977174 20549.4236174 20549.1772826\n",
      " 20518.0690478 20509.1887826 20620.0689304 20578.7358435 20582.468887\n",
      " 20658.5480826 20611.009387  20659.2504478 20752.3343478 20949.4086261\n",
      " 20955.8248522 20887.0930696 20998.0409652 21087.659587  21122.7266913\n",
      " 21061.8165565 21065.8180565 21064.0722826 21243.4083826 21366.5034174\n",
      " 21331.9361261 21585.6351435 21128.3430783 20960.5547565 20924.9161739\n",
      " 20908.5587609 20942.2326565 20825.2058913 20721.5878957 20710.1666261\n",
      " 20464.5950435 20291.0529957 20223.084413  20163.3969652 20339.6742739\n",
      " 20397.5843783 20438.5476174 20385.1766174 20395.8754739 20465.9829522\n",
      " 20396.8534391 20404.4619348 20437.4544348 20549.5563478 20637.1194913\n",
      " 20611.4661087 20635.119087  20572.3398957 20545.5882609 20539.0858522\n",
      " 20933.4844304 20902.1920783 21123.9788739 21164.477787  21232.2929\n",
      " 21244.4287478 21125.5951435 21189.3725783 21009.707187  21059.3778652\n",
      " 20979.1215783 20943.9770478 21103.1464174 21243.7607174 21163.9759\n",
      " 21168.0518304 21325.4549174 21384.3312348 21301.5182739 21546.664013\n",
      " 21546.7953609 21658.8191565 21620.8082478 21585.8598174]\n",
      "RMSE Değeri: 755.593732240723\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "veri = pd.read_csv(r\"C:/Users/ASUS/Desktop/AI CLUB DATATHON/CSV/bk_frx.csv\")\n",
    "veri['open'] = veri['open'].str.replace('\"', '').astype(float)\n",
    "veri['high'] = veri['high'].str.replace('\"', '').astype(float)\n",
    "veri['low'] = veri['low'].str.replace('\"', '').astype(float)\n",
    "veri['close'] = veri['close'].str.replace('\"', '').astype(float)\n",
    "\n",
    "veri['FuturePrice'] = veri['open'].shift(-1)\n",
    "\n",
    "veri.dropna(inplace=True)\n",
    "\n",
    "X = veri[['open', 'low', 'high', 'close']].tail(4886)  \n",
    "y = veri['FuturePrice'].tail(4886)  \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = scaler.fit_transform(y.values.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "\n",
    "gelecek_veri = X.tail(89)[::-1]\n",
    "rf_tahminler = rf_model.predict(gelecek_veri)\n",
    "\n",
    "\n",
    "rf_mse = mean_squared_error(y[-88:], rf_tahminler[:-1])\n",
    "rf_rmse = np.sqrt(rf_mse)\n",
    "\n",
    "print(\"Random Forest Regressor ile Gelecekteki Fiyat Tahminleri:\")\n",
    "np.set_printoptions(precision=7)\n",
    "print(rf_tahminler)\n",
    "print(\"RMSE Değeri:\", rf_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78a2a49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor ile Gelecekteki Fiyat Tahminleri:\n",
      "[20151.4565261 20225.8049261 20243.7560261 20295.4697391 20357.227413\n",
      " 20385.3243261 20366.6554217 20403.5231435 20367.456413  20464.727313\n",
      " 20475.1763783 20522.1703261 20519.073513  20529.4518348 20483.1738478\n",
      " 20548.1472391 20603.2859043 20577.7106391 20564.5984391 20589.9935043\n",
      " 20664.1672348 20590.4345565 20607.800813  20755.3235478 20698.3918696\n",
      " 20693.6433    20772.6704478 20964.967813  20963.2992348 20898.903313\n",
      " 21131.7666478 21139.7885435 21107.4140696 21087.9706739 21076.4906435\n",
      " 21373.1450087 21392.2588826 21602.0374913 21097.6911043 21158.5127522\n",
      " 21080.2674696 21061.2718087 20982.3027304 20948.5813652 20999.264113\n",
      " 20948.5373522 20864.6305174 20804.176413  20734.3585913 20511.3820609\n",
      " 20451.2531    20241.4042087 20177.9871739 20402.6901217 20415.1128609\n",
      " 20472.8635043 20434.1188913 20426.1988478 20494.9251    20492.2776348\n",
      " 20464.8943783 20567.6989391 20661.6928261 20669.6140217 20657.1712348\n",
      " 20632.3649304 20612.705387  20549.9049957 20805.6772348 20942.0169696\n",
      " 21091.9694087 20989.9384174 21155.9701348 21178.9933348 21255.9417304\n",
      " 21273.0365348 21177.1491652 21200.3836739 21155.6862391 21091.6214522\n",
      " 21089.4786391 21095.3839913 20958.4156304 21132.0198957 21207.7163391\n",
      " 21173.2870783 21426.0171174 21389.2023957 21327.4067   ]\n",
      "RMSE Değeri: 653.8977609132194\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "veri = pd.read_csv(r\"C:/Users/ASUS/Desktop/AI CLUB DATATHON/CSV/bk_frx.csv\")\n",
    "veri['open'] = veri['open'].str.replace('\"', '').astype(float)\n",
    "veri['high'] = veri['high'].str.replace('\"', '').astype(float)\n",
    "veri['low'] = veri['low'].str.replace('\"', '').astype(float)\n",
    "veri['close'] = veri['close'].str.replace('\"', '').astype(float)\n",
    "\n",
    "\n",
    "veri['FuturePrice'] = veri['high'].shift(-1)\n",
    "\n",
    "\n",
    "veri.dropna(inplace=True)\n",
    "\n",
    "\n",
    "X = veri[['open', 'low', 'high', 'close']].tail(4886) \n",
    "y = veri['FuturePrice'].tail(4886)  \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = scaler.fit_transform(y.values.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "gelecek_veri = X.tail(89)[::-1]\n",
    "rf_tahminler = rf_model.predict(gelecek_veri)\n",
    "\n",
    "rf_mse = mean_squared_error(y[-88:], rf_tahminler[:-1])\n",
    "rf_rmse = np.sqrt(rf_mse)\n",
    "\n",
    "print(\"Random Forest Regressor ile Gelecekteki Fiyat Tahminleri:\")\n",
    "np.set_printoptions(precision=7)\n",
    "print(rf_tahminler)\n",
    "print(\"RMSE Değeri:\", rf_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4f04562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor ile Gelecekteki Fiyat Tahminleri:\n",
      "[20079.6389913 20125.6234043 20164.3304565 20211.5209652 20232.4790087\n",
      " 20292.4473565 20238.6922217 20325.6707522 20303.2667304 20429.2191565\n",
      " 20409.0980522 20411.1703522 20420.9949391 20505.5774087 20429.9118435\n",
      " 20484.321413  20435.8628217 20465.9126696 20518.6098783 20558.5596652\n",
      " 20535.3564957 20602.1438696 20625.9201304 20630.1755696 20756.8573217\n",
      " 20953.3949174 20878.3989957 20868.0948739 21008.7312957 21061.5976435\n",
      " 21035.767287  21009.0541348 21235.1530565 21295.9373739 21043.9871261\n",
      " 21089.0624739 20996.5940652 20935.2389609 20898.9125304 20825.0752348\n",
      " 20881.8801739 20942.7444522 20836.4577913 20667.0661043 20270.0193696\n",
      " 20173.8752957 20153.6928957 20332.5669739 20363.2424522 20349.7180043\n",
      " 20325.5294957 20332.2517391 20328.782313  20393.463513  20505.176913\n",
      " 20544.7665304 20563.388887  20580.542913  20540.3537043 20479.2331826\n",
      " 20522.017087  20645.213513  20860.8930957 20872.6602478 20944.6817174\n",
      " 20921.470713  21140.7215739 21143.2844696 21142.0668522 21125.3043348\n",
      " 20983.143587  20973.1913391 20942.9055261 20930.4201087 21157.0363565\n",
      " 21310.7824435 21317.0903652 21390.4098739 21493.6176957 21476.4933957\n",
      " 21535.6934739 21592.0006739 21548.8319435 21532.7863087 21405.0682913\n",
      " 21505.138513  21516.162513  21533.9241957 21418.6833   ]\n",
      "RMSE Değeri: 802.0540051505249\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "veri = pd.read_csv(r\"C:/Users/ASUS/Desktop/AI CLUB DATATHON/CSV/bk_frx.csv\")\n",
    "veri['open'] = veri['open'].str.replace('\"', '').astype(float)\n",
    "veri['high'] = veri['high'].str.replace('\"', '').astype(float)\n",
    "veri['low'] = veri['low'].str.replace('\"', '').astype(float)\n",
    "veri['close'] = veri['close'].str.replace('\"', '').astype(float)\n",
    "\n",
    "veri['FuturePrice'] = veri['low'].shift(-1)\n",
    "\n",
    "veri.dropna(inplace=True)\n",
    "\n",
    "X = veri[['open', 'low', 'high', 'close']].tail(4886)  \n",
    "y = veri['FuturePrice'].tail(4886)  \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = scaler.fit_transform(y.values.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "gelecek_veri = X.tail(89)[::-1]\n",
    "rf_tahminler = rf_model.predict(gelecek_veri)\n",
    "\n",
    "rf_mse = mean_squared_error(y[-88:], rf_tahminler[:-1])\n",
    "rf_rmse = np.sqrt(rf_mse)\n",
    "\n",
    "print(\"Random Forest Regressor ile Gelecekteki Fiyat Tahminleri:\")\n",
    "np.set_printoptions(precision=7)\n",
    "print(rf_tahminler)\n",
    "print(\"RMSE Değeri:\", rf_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00f33337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor ile Gelecekteki Fiyat Tahminleri:\n",
      "[20095.2958826 20167.8003435 20197.4672087 20225.9653087 20256.1234609\n",
      " 20329.301713  20330.298113  20273.0673304 20359.5748522 20361.8994783\n",
      " 20459.1344304 20449.7552739 20481.4326826 20456.4867348 20471.9081217\n",
      " 20470.2250261 20535.6602087 20566.3861522 20512.3369826 20523.103587\n",
      " 20536.2121    20546.0002783 20576.7034087 20566.7244304 20652.2927\n",
      " 20666.1579609 20635.4935435 20678.7569826 20828.561713  20961.3684217\n",
      " 20892.6306478 20895.3182087 21011.4428217 21084.8330739 21096.2363696\n",
      " 21056.7769478 21059.7585435 21304.3867261 21339.7204435 21362.8671565\n",
      " 21423.552387  21081.2873739 21113.4975478 21040.7610391 21001.9961478\n",
      " 20932.5456391 20915.7672217 20897.2068522 20949.7234    20860.2944261\n",
      " 20797.341487  20712.1829304 20451.852     20398.9992478 20217.2509565\n",
      " 20164.9818957 20271.0466478 20357.1907739 20394.8138609 20397.7060478\n",
      " 20376.6461522 20416.4175826 20461.8574783 20470.2192652 20413.4707826\n",
      " 20591.3032957 20575.6079217 20612.1366739 20611.3937522 20613.8617087\n",
      " 20552.2662609 20542.2040957 20684.9347087 20886.8243826 20936.2581739\n",
      " 21018.7976087 20933.9584348 21095.4077261 21140.217613  21176.8698783\n",
      " 21220.3923261 21109.7949217 21143.0966652 21125.1918826 21059.5281087\n",
      " 21069.3022304 20994.3666826 20946.4855609 20966.0842696]\n",
      "RMSE Değeri: 549.0391426919456\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "veri = pd.read_csv(r\"C:/Users/ASUS/Desktop/AI CLUB DATATHON/CSV/bk_frx.csv\")\n",
    "veri['open'] = veri['open'].str.replace('\"', '').astype(float)\n",
    "veri['high'] = veri['high'].str.replace('\"', '').astype(float)\n",
    "veri['low'] = veri['low'].str.replace('\"', '').astype(float)\n",
    "veri['close'] = veri['close'].str.replace('\"', '').astype(float)\n",
    "\n",
    "veri['FuturePrice'] = veri['close'].shift(-1)\n",
    "\n",
    "veri.dropna(inplace=True)\n",
    "\n",
    "X = veri[['open', 'low', 'high', 'close']].tail(4886)  \n",
    "y = veri['FuturePrice'].tail(4886)  \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = scaler.fit_transform(y.values.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "gelecek_veri = X.tail(89)[::-1]\n",
    "rf_tahminler = rf_model.predict(gelecek_veri)\n",
    "\n",
    "rf_mse = mean_squared_error(y[-88:], rf_tahminler[:-1])\n",
    "rf_rmse = np.sqrt(rf_mse)\n",
    "\n",
    "print(\"Random Forest Regressor ile Gelecekteki Fiyat Tahminleri:\")\n",
    "np.set_printoptions(precision=7)\n",
    "print(rf_tahminler)\n",
    "print(\"RMSE Değeri:\", rf_rmse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
